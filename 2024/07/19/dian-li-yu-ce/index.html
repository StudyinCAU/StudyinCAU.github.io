<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>电力需求预测赛 | StudyinCAU</title><meta name="author" content="小楼一夜听春雨 &amp; Rico"><meta name="copyright" content="小楼一夜听春雨 &amp; Rico"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="题目 题目链接点这里 ## 赛题背景 随着全球经济的快速发展和城市化进程的加速，电力系统面临着越来越大的挑战。电力需求的准确预测对于电网的稳定运行、能源的有效管理以及可再生能源的整合至关重要。 然而，电力需求受到多种因素的影响，为了提高电力需求预测的准确性和可靠性，推动智能电网和可持续能源系统的发展，本场以“电力需求预测”为赛题的数据算法挑战赛。选手需要根据历史数据构建有效的模型，能够准确的预测未">
<meta property="og:type" content="article">
<meta property="og:title" content="电力需求预测赛">
<meta property="og:url" content="https://studyincau.github.io/2024/07/19/dian-li-yu-ce/index.html">
<meta property="og:site_name" content="StudyinCAU">
<meta property="og:description" content="题目 题目链接点这里 ## 赛题背景 随着全球经济的快速发展和城市化进程的加速，电力系统面临着越来越大的挑战。电力需求的准确预测对于电网的稳定运行、能源的有效管理以及可再生能源的整合至关重要。 然而，电力需求受到多种因素的影响，为了提高电力需求预测的准确性和可靠性，推动智能电网和可持续能源系统的发展，本场以“电力需求预测”为赛题的数据算法挑战赛。选手需要根据历史数据构建有效的模型，能够准确的预测未">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://studyincau.github.io/images/xunfei.png">
<meta property="article:published_time" content="2024-07-19T15:33:35.000Z">
<meta property="article:modified_time" content="2024-07-22T15:46:56.873Z">
<meta property="article:author" content="小楼一夜听春雨 &amp; Rico">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://studyincau.github.io/images/xunfei.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://studyincau.github.io/2024/07/19/dian-li-yu-ce/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '电力需求预测赛',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-07-22 23:46:56'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.1.1">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">120</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">212</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">56</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/../images/xunfei.png')"><nav id="nav"><span id="blog-info"><a href="/" title="StudyinCAU"><span class="site-name">StudyinCAU</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">电力需求预测赛</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-19T15:33:35.000Z" title="发表于 2024-07-19 23:33:35">2024-07-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-22T15:46:56.873Z" title="更新于 2024-07-22 23:46:56">2024-07-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/">项目实战</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>26分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="电力需求预测赛"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="题目">题目</h1>
<p>题目链接点<a target="_blank" rel="noopener" href="https://challenge.xfyun.cn/topic/info?type=electricity-demand&amp;ch=dw24_uGS8Gs">这里</a>
## 赛题背景</p>
<p>随着全球经济的快速发展和城市化进程的加速，电力系统面临着越来越大的挑战。电力需求的准确预测对于电网的稳定运行、能源的有效管理以及可再生能源的整合至关重要。</p>
<p>然而，电力需求受到多种因素的影响，为了提高电力需求预测的准确性和可靠性，推动智能电网和可持续能源系统的发展，本场以“电力需求预测”为赛题的数据算法挑战赛。选手需要根据历史数据构建有效的模型，能够准确的预测未来电力需求。</p>
<h2 id="赛题任务">赛题任务</h2>
<p>给定多个房屋对应电力消耗历史N天的相关序列数据等信息，预测房屋对应电力的消耗。</p>
<h2 id="评审规则">评审规则</h2>
<h3 id="数据说明">数据说明</h3>
<p>赛题数据由训练集和测试集组成，为了保证比赛的公平性，将每日日期进行脱敏，用1-N进行标识，即1为数据集最近一天，其中1-10为测试集数据。数据集由字段<code>id</code>（房屋id）、
<code>dt</code>（日标识）、<code>type</code>（房屋类型）、<code>target</code>（实际电力消耗）组成。</p>
<p><span class="math display">\[
\begin{array}{|l|l|}
\hline \text { 特征字段 } &amp; \text { 字段描述 } \\
\hline \text { id } &amp; \text { 房屋id } \\
\hline \text { dt } &amp; \text { 日标识 } \\
\hline \text { type } &amp; \text { 房屋类型 } \\
\hline \text { target } &amp; \text { 实际电力消耗，预测目标 } \\
\hline
\end{array}
\]</span></p>
<h3 id="评审规则-1">评审规则</h3>
<p>预测结果以 <strong>MES</strong>(Mean Square Error)
作为评判标准，具体公式如下： <span class="math display">\[
\frac{1}{n} \sum_{n=1}^n\left(y_i-\bar{y}_i\right)^2
\]</span> 其中，<span class="math inline">\(y_i\)</span>
是真实电力消耗，<span class="math inline">\(\bar{y}_i\)</span>
是预测电力消耗。</p>
<h2 id="数据下载">数据下载</h2>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 66%">
</colgroup>
<thead>
<tr class="header">
<th>文件名</th>
<th>下载</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>电力需求预测挑战赛数据集.zip</td>
<td><a target="_blank" rel="noopener" href="https://ai-contest-static.xfyun.cn/2024/%E7%94%B5%E5%8A%9B%E9%9C%80%E6%B1%82%E9%A2%84%E6%B5%8B%E6%8C%91%E6%88%98%E8%B5%9B/dataset.zip">下载文件</a></td>
</tr>
<tr class="even">
<td>电力需求预测挑战赛提交示例.csv</td>
<td><a target="_blank" rel="noopener" href="https://ai-contest-static.xfyun.cn/2024/%E7%94%B5%E5%8A%9B%E9%9C%80%E6%B1%82%E9%A2%84%E6%B5%8B%E6%8C%91%E6%88%98%E8%B5%9B/sample_submit.csv">下载文件</a></td>
</tr>
</tbody>
</table>
<h1 id="task-1">Task 1</h1>
<p>baseline 的代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 导入需要用到的相关库</span></span><br><span class="line"><span class="comment"># 导入 pandas 库，用于数据处理和分析</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 导入 numpy 库，用于科学计算和多维数组操作</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 读取训练集和测试集</span></span><br><span class="line"><span class="comment"># 使用 read_csv() 函数从文件中读取训练集数据，文件名为 'train.csv'</span></span><br><span class="line">train = pd.read_csv(<span class="string">'../dataset/train.csv'</span>)</span><br><span class="line"><span class="comment"># 使用 read_csv() 函数从文件中读取测试集数据，文件名为 'train.csv'</span></span><br><span class="line">test = pd.read_csv(<span class="string">'../dataset/test.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 计算训练数据最近11-20单位时间内对应id的目标均值</span></span><br><span class="line">target_mean = train[train[<span class="string">'dt'</span>]&lt;=<span class="number">20</span>].groupby([<span class="string">'id'</span>])[<span class="string">'target'</span>].mean().reset_index()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 将target_mean作为测试集结果进行合并</span></span><br><span class="line">test = test.merge(target_mean, on=[<span class="string">'id'</span>], how=<span class="string">'left'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 保存结果文件到本地</span></span><br><span class="line">test[[<span class="string">'id'</span>,<span class="string">'dt'</span>,<span class="string">'target'</span>]].to_csv(<span class="string">'submit.csv'</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>将 baseline 的结果提交后可得 MSE 得分为：373.89846</p>
<p>接下来我们来分析一下 baseline 代码：</p>
<p>可以看到 baseline 主要通过了如下几个步骤对数据进行处理：</p>
<ol type="1">
<li><strong>导入库</strong>：首先，代码导入了需要用到的库，包括
pandas（用于数据处理和分析）。</li>
<li><strong>读取数据</strong>：代码通过使用 pd.read_csv
函数从文件中读取训练集和测试集数据，并将其存储在 train.csv 和 test.csv
两个数据框中。</li>
<li><strong>计算最近时间的用电均值</strong>：
<ul>
<li>计算训练数据最近11-20单位时间内对应id的目标均值，可以用来反映最近的用电情况。</li>
</ul></li>
<li><strong>将用电均值直接作为预测结果</strong>：
<ul>
<li>这里使用merge函数根据'id'列将test和target_mean两个DataFrame进行左连接，这意味着测试集的所有行都会保留。</li>
</ul></li>
<li><strong>保存结果文件到本地</strong>：
<ul>
<li>使用to_csv()函数将测试集的'id'、'dt'和'target'列保存为CSV文件，文件名为'submit.csv'。index=None参数表示在保存时不包含行索引。</li>
</ul></li>
</ol>
<p>我们可以看到，上述代码中的选取的是训练数据最近11-20单位时间内对应id的目标均值，那么如果我们选取所有数据再取平均值呢？</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">'../dataset/train.csv'</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">'../dataset/test.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算训练数据最近11-506单位时间内对应id的目标均值</span></span><br><span class="line">target_mean = train[train[<span class="string">'dt'</span>]&lt;=<span class="number">506</span>].groupby([<span class="string">'id'</span>])[<span class="string">'target'</span>].mean().reset_index()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将target_mean作为测试集结果进行合并</span></span><br><span class="line">test = test.merge(target_mean, on=[<span class="string">'id'</span>], how=<span class="string">'left'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存结果文件到本地</span></span><br><span class="line">test[[<span class="string">'id'</span>,<span class="string">'dt'</span>,<span class="string">'target'</span>]].to_csv(<span class="string">'submit.csv'</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>将结果提交后可得 MSE 得分为：629.76628</p>
<p>效果更差了，这是因为这是一个时序预测的题目，当选取所有数据进行预测时，就会导致效果较差，应当选取待预测附近的一些数据进行预测。</p>
<h1 id="task-2">Task 2</h1>
<h2 id="基础概念">基础概念</h2>
<ol type="1">
<li><p>GBDT</p>
<p>GBDT (Gradient Boosting Decision Tree)
是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。</p></li>
<li><p>LightGBM</p>
<p>LightGBM（Light Gradient Boosting
Machine）是一个实现GBDT算法的框架，支持高效率的并行训练，并且具有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据等优点。</p></li>
</ol>
<p>在 Task 2 中，我们将使用 <strong>LightGBM 模型</strong>。</p>
<h2 id="lightgbm">LightGBM</h2>
<p>我们先来看代码：</p>
<h3 id="导入模块">导入模块</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_log_error, mean_absolute_error, mean_squared_error</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.interpolate <span class="keyword">import</span> UnivariateSpline</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimHei'</span>]  <span class="comment"># 设置字体为黑体</span></span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span>    <span class="comment"># 解决负号显示问题</span></span><br></pre></td></tr></tbody></table></figure>
<p>导入必要的库，包括 numpy、pandas、lightgbm、sklearn 和 matplotlib
等。</p>
<p>设置中文字体，避免出现乱码问题。</p>
<p>设置忽略警告信息，以便代码运行时不显示警告。</p>
<h3 id="读取数据">读取数据</h3>
<p>读取训练数据和测试数据，并存储在 train 和 test 中。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">'../dataset/train.csv'</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">'../dataset/test.csv'</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'****************train*****************'</span>)</span><br><span class="line"><span class="built_in">print</span>(train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'\n*************test*************'</span>)</span><br><span class="line"><span class="built_in">print</span>(test)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>****************train*****************
                 id   dt  type  target
0        00037f39cf   11     2  44.050
1        00037f39cf   12     2  50.672
2        00037f39cf   13     2  39.042
3        00037f39cf   14     2  35.900
4        00037f39cf   15     2  53.888
...             ...  ...   ...     ...
2877300  fff81139a7  502     5  28.552
2877301  fff81139a7  503     5  22.818
2877302  fff81139a7  504     5  21.282
2877303  fff81139a7  505     5  22.021
2877304  fff81139a7  506     5  18.145

[2877305 rows x 4 columns]

*************test*************
               id  dt  type
0      00037f39cf   1     2
1      00037f39cf   2     2
2      00037f39cf   3     2
3      00037f39cf   4     2
4      00037f39cf   5     2
...           ...  ..   ...
58315  fff81139a7   6     5
58316  fff81139a7   7     5
58317  fff81139a7   8     5
58318  fff81139a7   9     5
58319  fff81139a7  10     5

[58320 rows x 3 columns]</code></pre>
<h3 id="数据可视化">数据可视化</h3>
<h4 id="柱状图">柱状图</h4>
<p>不同 type 类型对应 target 的柱状图：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">type_target_df = train.groupby(<span class="string">'type'</span>)[<span class="string">'target'</span>].mean().reset_index()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Seaborn绘制柱状图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">sns.barplot(x=<span class="string">'type'</span>, y=<span class="string">'target'</span>, data=type_target_df, palette=[<span class="string">"#b1283a"</span>, <span class="string">"#006a8e"</span>])</span><br><span class="line">plt.xlabel(<span class="string">'类型'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'平均目标值'</span>)</span><br><span class="line">plt.title(<span class="string">'不同类型的目标值柱状图'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="../../../../images/dianli/output_12_1.png"></p>
<h4 id="折线图">折线图</h4>
<p><code>id</code> 为 <code>00037f39cf</code> 的按dt为序列关于
<code>target</code> 的折线图：</p>
<p>并且由于数据较多，原始折线图并不美观且为了直观的看出
<code>target</code> 的变化趋势，我们对其进行了平滑处理。</p>
<p><code>UnivariateSpline</code> 是 <code>scipy</code>
库中用于一元函数插值和平滑的工具。其基本思想是通过一个平滑函数来逼近数据点，使得曲线更加平滑，同时尽可能贴合原始数据。</p>
<p>数学原理：</p>
<p>好的，让我们详细解释一下使用 <code>UnivariateSpline</code>
进行平滑处理的原理以及代码中各个参数的作用和计算过程。</p>
<h5 id="平滑处理的原理">平滑处理的原理</h5>
<p><code>UnivariateSpline</code> 是 <code>scipy</code>
库中用于一元函数插值和平滑的工具。</p>
<p>其基本思想是通过一个平滑函数来逼近数据点，使得曲线更加平滑，同时尽可能贴合原始数据。</p>
<h6 id="数学原理">数学原理</h6>
<p>假设我们有一组数据点 <span class="math inline">\((x_i,
y_i)\)</span>，希望找到一个平滑函数 <span class="math inline">\(s(x)\)</span> 来逼近这些数据点。</p>
<p><code>UnivariateSpline</code>
使用最小二乘法来找到一个平滑的三次样条函数（cubic
spline），即最小化以下目标函数：</p>
<p><span class="math display">\[
\sum_{i=1}^{n} \left( y_i - s(x_i) \right)^2 + \lambda \int_{a}^{b}
\left( s''(x) \right)^2 dx
\]</span></p>
<p>其中： - <span class="math inline">\(\left( y_i - s(x_i)
\right)^2\)</span> 是数据点与平滑曲线之间的误差。 - <span class="math inline">\(\int_{a}^{b} \left( s''(x) \right)^2
dx\)</span> 是平滑曲线的二阶导数的积分，用于控制曲线的平滑程度。 - <span class="math inline">\(\lambda\)</span>
是平滑参数，控制拟合的平滑程度。较大的 <span class="math inline">\(\lambda\)</span>
值会产生更平滑的曲线，但可能会偏离数据点；较小的 <span class="math inline">\(\lambda\)</span>
值会产生更贴合数据点的曲线，但可能会有较大的波动。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 筛选特定 id 的数据</span></span><br><span class="line">specific_id_df = train[train[<span class="string">'id'</span>] == <span class="string">'00037f39cf'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建子图</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">18</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 子图1：原始折线图</span></span><br><span class="line">sns.lineplot(ax=axes[<span class="number">0</span>], x=<span class="string">'dt'</span>, y=<span class="string">'target'</span>, data=specific_id_df, marker=<span class="string">'o'</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_xlabel(<span class="string">'日期'</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_ylabel(<span class="string">'目标值'</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">"ID 为 '00037f39cf' 的目标值折线图"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 子图2：平滑后的折线图</span></span><br><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">x = specific_id_df[<span class="string">'dt'</span>]</span><br><span class="line">y = specific_id_df[<span class="string">'target'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用UnivariateSpline进行平滑处理</span></span><br><span class="line">spl = UnivariateSpline(x, y)</span><br><span class="line">xs = np.linspace(x.<span class="built_in">min</span>(), x.<span class="built_in">max</span>(), <span class="number">100</span>)</span><br><span class="line">ys = spl(xs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制平滑后的折线图</span></span><br><span class="line">sns.lineplot(ax=axes[<span class="number">1</span>], x=xs, y=ys)</span><br><span class="line">axes[<span class="number">1</span>].set_xlabel(<span class="string">'日期'</span>)</span><br><span class="line">axes[<span class="number">1</span>].set_ylabel(<span class="string">'目标值'</span>)</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">"ID 为 '00037f39cf' 的目标值平滑折线图"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调整布局</span></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="../../../../images/dianli/output_14_0.png"></p>
<p>部分代码解释：</p>
<ul>
<li><code>UnivariateSpline(x, y)</code>：创建一个三次样条插值对象，自动选择平滑参数
<span class="math inline">\(\lambda\)</span>。</li>
<li><code>np.linspace(x.min(), x.max(), 100)</code>：在 <code>x</code>
的范围内生成 100 个均匀分布的点，用于绘制平滑曲线。</li>
<li><code>spl(xs)</code>：使用三次样条插值对象对新的自变量
<code>xs</code> 进行插值，得到平滑曲线的因变量 <code>ys</code>。</li>
</ul>
<h3 id="数据预处理">数据预处理</h3>
<h4 id="合并和排序数据">合并和排序数据</h4>
<p>合并训练数据和测试数据，并按 <code>id</code> 和 <code>dt</code>
排序。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">data = pd.concat([test, train], axis=<span class="number">0</span>, ignore_index=<span class="literal">True</span>)</span><br><span class="line">data = data.sort_values([<span class="string">'id'</span>,<span class="string">'dt'</span>], ascending=<span class="literal">False</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>                 id   dt  type  target
0        fff81139a7  506     5  18.145
1        fff81139a7  505     5  22.021
2        fff81139a7  504     5  21.282
3        fff81139a7  503     5  22.818
4        fff81139a7  502     5  28.552
...             ...  ...   ...     ...
2935620  00037f39cf    5     2     NaN
2935621  00037f39cf    4     2     NaN
2935622  00037f39cf    3     2     NaN
2935623  00037f39cf    2     2     NaN
2935624  00037f39cf    1     2     NaN

[2935625 rows x 4 columns]</code></pre>
<h4 id="历史平移">历史平移</h4>
<p>时间序列数据具有趋势和季节性模式。历史平移特征可以帮助模型识别这些模式。</p>
<p>在本题中即电力需求可能具有每日、每周或每月的周期性变化。</p>
<p>并且一般具有时间依赖性，即当前值可能依赖于过去的值。通过引入历史平移特征，模型能够捕捉这种时间依赖性，从而提高预测性能。</p>
<p>进行历史平移（lagging）是时间序列数据处理中一种常见的特征工程方法，特别是在预测任务中。</p>
<p>其主要目的是通过引入过去一段时间的目标值作为特征，帮助模型更好地理解和预测未来的趋势和模式。</p>
<p>假设我们有一个时间序列数据集 <span class="math inline">\(\{y_t\}\)</span>，其中 <span class="math inline">\(y_t\)</span> 是时间 <span class="math inline">\(t\)</span> 的目标值。为了预测未来的目标值 <span class="math inline">\(y_{t+k}\)</span>（这里 <span class="math inline">\(k\)</span>
是预测的步长），我们可以引入过去的目标值作为特征。</p>
<p>对于每个时间 <span class="math inline">\(t\)</span>，生成的历史平移特征可以表示为：</p>
<p><span class="math display">\[
\text{lag}_k(y_t) = y_{t-k}
\]</span></p>
<p>其中，<span class="math inline">\(\text{lag}_k(y_t)\)</span> 表示第
<span class="math inline">\(k\)</span> 个滞后的目标值。</p>
<p>例如，对于时间 <span class="math inline">\(t\)</span> 的目标值 <span class="math inline">\(y_t\)</span>，引入前 10 到 30
天的历史平移特征可以表示为：</p>
<p><span class="math display">\[
\{ y_{t-10}, y_{t-11}, \ldots, y_{t-30} \}
\]</span></p>
<p>下面对每个 id 进行分组，并生成前 10 到 30 天的 target 平移特征。</p>
<p><img src="../../../../images/dianli/fig1.png"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>,<span class="number">30</span>):</span><br><span class="line">    data[<span class="string">f'last<span class="subst">{i}</span>_target'</span>] = data.groupby([<span class="string">'id'</span>])[<span class="string">'target'</span>].shift(i)</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>使用 <code>groupby(['id'])</code> 对每个 id
进行分组，以处理不同房屋的时间序列数据。</li>
<li>使用 shift(i) 生成前 i 天的 target 平移特征。shift(i)
函数会将目标值向下移动 i 个位置，从而生成相应的历史特征。</li>
<li>生成的特征命名为 <code>last{i}_target</code>，例如
<code>last10_target</code> 表示前第 10 天的 <code>target</code>
值。</li>
</ul>
<h4 id="窗口统计">窗口统计</h4>
<p>生成窗口统计特征是一种常见的特征工程方法，特别适用于时间序列数据。这些特征可以帮助模型捕捉时间序列数据中的短期趋势和变化，从而提高预测性能。</p>
<p>计算前 10 到 12 天的 target
<strong>均值</strong>，生成窗口统计特征。</p>
<p><img src="../../../../images/dianli/fig2.png"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">data[<span class="string">f'win3_mean_target'</span>] = (data[<span class="string">'last10_target'</span>] + data[<span class="string">'last11_target'</span>] + data[<span class="string">'last12_target'</span>]) / <span class="number">3</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="数据分割">数据分割</h4>
<p>根据 <code>target</code>
是否为空，将数据重新划分为训练集和测试集。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">train = data[data.target.notnull()].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">test = data[data.target.isnull()].reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="确定输入特征">确定输入特征</h4>
<ol type="1">
<li><p>排除 <code>id</code> 列</p>
<ul>
<li><p>无预测价值：</p>
<p>id
是房屋的标识，它不包含任何关于目标变量的信息，不具有实际的预测价值。</p></li>
</ul></li>
<li><p>排除 <code>target</code> 列</p>
<ul>
<li><p>防止数据泄漏：</p>
<p><code>target</code> 列是我们要预测的目标变量。如果在模型训练过程中将
<code>target</code>
作为输入特征，会导致数据泄漏，模型直接学习目标值，而不是数据中的实际模式和关系。</p></li>
</ul></li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">train_cols = [f <span class="keyword">for</span> f <span class="keyword">in</span> data.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'id'</span>,<span class="string">'target'</span>]]</span><br></pre></td></tr></tbody></table></figure>
<h3 id="定义模型训练函数">定义模型训练函数</h3>
<h4 id="代码">代码</h4>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">time_model</span>(<span class="params">lgb, train_df, test_df, cols</span>):</span><br><span class="line">    <span class="comment"># 训练集和验证集切分</span></span><br><span class="line">    trn_x, trn_y = train_df[train_df.dt&gt;=<span class="number">31</span>][cols], train_df[train_df.dt&gt;=<span class="number">31</span>][<span class="string">'target'</span>]</span><br><span class="line">    val_x, val_y = train_df[train_df.dt&lt;=<span class="number">30</span>][cols], train_df[train_df.dt&lt;=<span class="number">30</span>][<span class="string">'target'</span>]</span><br><span class="line">    <span class="comment"># 构建模型输入数据</span></span><br><span class="line">    train_matrix = lgb.Dataset(trn_x, label=trn_y)</span><br><span class="line">    valid_matrix = lgb.Dataset(val_x, label=val_y)</span><br><span class="line">    <span class="comment"># lightgbm参数</span></span><br><span class="line">    lgb_params = {</span><br><span class="line">        <span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,</span><br><span class="line">        <span class="string">'objective'</span>: <span class="string">'regression'</span>,</span><br><span class="line">        <span class="string">'metric'</span>: <span class="string">'mse'</span>,</span><br><span class="line">        <span class="string">'min_child_weight'</span>: <span class="number">5</span>,</span><br><span class="line">        <span class="string">'num_leaves'</span>: <span class="number">2</span> ** <span class="number">5</span>,</span><br><span class="line">        <span class="string">'lambda_l2'</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">'feature_fraction'</span>: <span class="number">0.8</span>,</span><br><span class="line">        <span class="string">'bagging_fraction'</span>: <span class="number">0.8</span>,</span><br><span class="line">        <span class="string">'bagging_freq'</span>: <span class="number">4</span>,</span><br><span class="line">        <span class="string">'learning_rate'</span>: <span class="number">0.05</span>,</span><br><span class="line">        <span class="string">'seed'</span>: <span class="number">666</span>,</span><br><span class="line">        <span class="string">'nthread'</span> : <span class="number">16</span>,</span><br><span class="line">        <span class="string">'verbose'</span> : -<span class="number">1</span>,</span><br><span class="line">        <span class="string">'early_stopping_rounds'</span>: <span class="number">500</span>,</span><br><span class="line">        <span class="string">'verbose_eval'</span>:<span class="number">500</span></span><br><span class="line">    }</span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    model = lgb.train(lgb_params, train_matrix, <span class="number">50000</span>, valid_sets=[valid_matrix],</span><br><span class="line">                      categorical_feature=[])</span><br><span class="line">    <span class="comment"># 验证集和测试集结果预测</span></span><br><span class="line">    val_pred = model.predict(val_x, num_iteration=model.best_iteration)</span><br><span class="line">    test_pred = model.predict(test_df[cols], num_iteration=model.best_iteration)</span><br><span class="line">    <span class="comment"># 离线分数评估</span></span><br><span class="line">    score = mean_squared_error(val_pred, val_y)</span><br><span class="line">    <span class="built_in">print</span>(score)</span><br><span class="line">       </span><br><span class="line">    <span class="keyword">return</span> val_pred, test_pred</span><br></pre></td></tr></tbody></table></figure>
<p>在上述代码中：</p>
<ul>
<li>切分训练集和验证集。dt 大于等于 31 作为训练集，小于等于 30
作为验证集。</li>
<li>创建 LightGBM 数据矩阵。</li>
<li>定义 LightGBM 参数，包括：</li>
</ul>
<p><span class="math display">\[
\begin{array}{|c|c|}
\hline \text { 参数 } &amp; \text { 值 } \\
\hline \text { boosting\_type (提升方法) } &amp; \text { gbdt } \\
\hline \text { objective (目标函数) } &amp; \text { regression } \\
\hline \text { metric (评估指标) } &amp; \text { mse } \\
\hline \text { min\_child\_weight (叶子节点最小权重) } &amp; 5 \\
\hline \text { num\_leaves (叶子节点数) } &amp; 32 \\
\hline \text { lambda\_12 (L2 正则化系数) } &amp; 10 \\
\hline \text { feature\_fraction (特征采样率) } &amp; 0.8 \\
\hline \text { bagging\_fraction (样本采样率) } &amp; 0.8 \\
\hline \text { bagging\_freq (采样频率) } &amp; 4 \\
\hline \text { learning\_rate (学习率) } &amp; 0.05 \\
\hline \text { seed (随机种子) } &amp; 2024 \\
\hline \text { nthread (线程数) } &amp; 16 \\
\hline \text { verbose (输出信息级别) } &amp; -1 \\
\hline
\end{array}
\]</span></p>
<ul>
<li>训练模型，并在验证集上进行预测和评估。</li>
</ul>
<hr>
<h4 id="参数详解"><strong>参数详解</strong>：</h4>
<ul>
<li><strong>boosting_type</strong>
<ul>
<li><strong>含义</strong>：指定提升（boosting）的类型。这里使用的是GBDT。</li>
</ul></li>
</ul>
<div>
<p style="color:red;">
GBDT详解
</p>
</div>
GBDT 即 Gradient Boosting Decision
Tree（梯度提升决策树），是一种集成学习方法。它通过构建多个决策树并逐步改进模型的预测能力来提升整体性能。
<hr>
<p><strong>工作原理</strong></p>
<p><strong>GBDT</strong>
通过逐步添加决策树，每一步的新树都是在前一步模型的残差上进行训练的。其核心思想是使用梯度下降法来最小化损失函数。</p>
<p>下面我们来详细地看看 <strong>GBDT</strong> 的工作原理：</p>
<ol type="1">
<li><p>初始化模型<br>初始化模型 <span class="math inline">\(F_0(x)\)</span> 是选择一个常数值 <span class="math inline">\(\gamma\)</span> ，使得损失函数 <span class="math inline">\(L\)</span> 的和最小化。 <span class="math display">\[
F_0(x) = \arg\min_{\gamma} \sum_{i=1}^n L(y_i, \gamma)
\]</span> 其中：</p>
<ul>
<li><p>$ L $ 是损失函数</p></li>
<li><p>$ y_i $ 是实际值</p></li>
<li><p>$ $ ：</p>
<ul>
<li><p>含义：<span class="math inline">\(\gamma\)</span>
是初始模型的预测值，它是一个常数。</p>
<p>在训练开始时，模型 <span class="math inline">\(F_0(x)\)</span>
对所有输入 <span class="math inline">\(x\)</span> 都输出这个常数值 <span class="math inline">\(\gamma\)</span> 。</p></li>
<li><p>来源：<span class="math inline">\(\gamma\)</span>
的值是通过最小化初始损失函数来确定的。</p>
<p>具体来说， <span class="math inline">\(\gamma\)</span> 是使损失函数
<span class="math inline">\(\sum_{i=1}^n L\left(y_i,
\gamma\right)\)</span> 最小化的常数。</p></li>
</ul></li>
</ul></li>
<li><p>计算残差<br>对于每个样本，计算当前模型的残差（也称为负梯度），即目标值和当前模型预测值之间的差异。
<span class="math display">\[
r_{i,m} = -\left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}
\right]_{F=F_{m-1}}
\]</span> 在平方误差情况下，残差可以简化为： <span class="math display">\[
r_{i,m} = y_i - F_{m-1}(x_i)
\]</span></p></li>
<li><p>拟合新树<br>用残差作为目标值，训练一个新的决策树 $ h_m(x) <span class="math inline">\(。\)</span>$ h_m(x) = <em>{h} </em>{i=1}^n (r_{im}
- h(x_i))^2 $$</p></li>
<li><p>更新模型<br>将新的决策树加入到模型中，更新后的模型为： <span class="math display">\[
F_m(x) = F_{m-1}(x) + \gamma_m h_m(x)
\]</span> 其中，$ _m $ 是学习率，控制新树对模型的贡献大小。</p></li>
<li><p>重复步骤 2 到
4<br>重复上述步骤，直到达到预定的迭代次数或误差满足要求。</p></li>
</ol>
<hr>
<p><strong>损失函数</strong></p>
<p>GBDT 的目标是最小化损失函数 $ L $，常见的损失函数包括：</p>
<ul>
<li>均方误差（MSE）</li>
</ul>
<p><span class="math display">\[
L(y, \hat{y}) = \frac{1}{2} (y - \hat{y})^2
\]</span></p>
<ul>
<li>交叉熵损失（分类问题）</li>
</ul>
<p><span class="math display">\[
L(y, \hat{y}) = -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]
\]</span></p>
<hr>
<p><strong>梯度下降</strong></p>
<p>在每次迭代中，GBDT 使用梯度下降法来优化模型。对于给定的损失函数 $ L
$，梯度的计算方式为：</p>
<p><span class="math display">\[
\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}
\]</span></p>
<p>这个梯度表示了当前模型的预测与实际值之间的差异。在平方误差情况下，梯度（残差）为：</p>
<p><span class="math display">\[
r_{i,m} = y_i - F_{m-1}(x_i)
\]</span></p>
<hr>
<p><strong>新树的训练</strong></p>
<p>新的决策树 $ h_m(x) $ 是通过最小化残差的平方和来训练的，即： <span class="math display">\[
h_m(x) = \arg\min_{h} \sum_{i=1}^n (r_{i,m} - h(x_i))^2
\]</span></p>
<hr>
<ul>
<li><p><strong>objective</strong></p>
<ul>
<li><p><strong>含义</strong>：定义模型的目标函数。这里使用的是回归（regression）。</p></li>
<li><p><strong>数学原理</strong>：目标函数是均方误差（MSE），即：</p>
<p><span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (\hat{y}_i - y_i)^2
\]</span></p></li>
</ul></li>
<li><p><strong>metric</strong></p>
<ul>
<li><strong>含义</strong>：评估模型性能的指标。这里使用的是均方误差（MSE）。</li>
<li><strong>数学原理</strong>：评估模型预测值与实际值之间的差异，均方误差的公式如上。</li>
</ul></li>
<li><p><strong>min_child_weight</strong></p>
<ul>
<li><strong>含义</strong>：一个叶子节点中最小的样本权重和。</li>
<li><strong>数学原理</strong>：控制叶子节点的最小 <a href="#Hessian">Hessian</a>
和（即二阶导数和），避免叶子节点样本数过少，防止过拟合。</li>
</ul></li>
<li><p><strong>num_leaves</strong></p>
<ul>
<li><p><strong>含义</strong>：树的最大叶子数。这里设置为 32。</p></li>
<li><p><strong>数学原理</strong>：叶子数越多，树的复杂度越高，可以捕捉更多的细节，但也可能导致过拟合。叶子数与树的深度
<span class="math inline">\(d\)</span> 关系为：</p>
<p><span class="math display">\[
\text{num\_leaves} = 2^d
\]</span></p></li>
</ul></li>
<li><p><strong>lambda_l2</strong></p>
<ul>
<li><p><strong>含义</strong>：L2 正则化系数（也叫
<code>reg_lambda</code>）。</p></li>
<li><p><strong>数学原理</strong>：增加权重的平方和惩罚项，防止过拟合。正则化项为：</p>
<p><span class="math display">\[
\Omega(f) = \gamma T + \frac{1}{2} \lambda \|w\|^2
\]</span></p>
<p>其中，<span class="math inline">\(\lambda\)</span> 为
<code>lambda_l2</code>。</p></li>
</ul></li>
<li><p><strong>feature_fraction</strong></p>
<ul>
<li><strong>含义</strong>：在每次迭代中随机选择的特征比例。这里设置为
0.8。</li>
<li><strong>数学原理</strong>：每棵树只使用部分特征进行训练，减少过拟合，提高模型的泛化能力。</li>
</ul></li>
<li><p><strong>bagging_fraction</strong></p>
<ul>
<li><strong>含义</strong>：在每次迭代中随机选择的样本比例。这里设置为
0.8。</li>
<li><strong>数学原理</strong>：每次迭代使用部分数据样本进行训练，类似于
Bagging 方法，减少过拟合。</li>
</ul></li>
<li><p><strong>bagging_freq</strong></p>
<ul>
<li><strong>含义</strong>：执行 bagging 的频率。这里设置为 4。</li>
<li><strong>数学原理</strong>：每 4 次迭代进行一次样本的重采样。</li>
</ul></li>
<li><p><strong>learning_rate</strong></p>
<ul>
<li><p><strong>含义</strong>：学习率，控制每棵树对模型的贡献大小。这里设置为
0.05。</p></li>
<li><p><strong>数学原理</strong>：较小的学习率可以让模型更平滑地逼近目标函数，但需要更多的树来收敛。更新模型的公式为：</p>
<p><span class="math display">\[
F_m(x) = F_{m-1}(x) + \gamma h_m(x)
\]</span></p>
<p>其中，<span class="math inline">\(\gamma\)</span> 是学习率。</p></li>
</ul></li>
<li><p><strong>seed</strong></p>
<p>随机种子，用于重现结果。这里设置为 2024。</p>
<p>设置随机种子可以确保每次运行结果一致，便于我们进行调试和验证模型效果。</p></li>
<li><p><strong>nthread</strong></p>
<p>线程数量，控制并行计算的线程数。这里设置为
16，多线程可以加速训练过程。</p></li>
<li><p><strong>verbose</strong></p>
<p>控制训练过程的输出信息。设置为 -1
表示不输出详细信息，减少日志信息的输出，提高训练过程的简洁性。</p></li>
</ul>
<h4 style="color:red;">
LightGBM
</h4>
<p>在看完了上述的代码后及其参数后，我们来看一下 LightGBM
的具体原理。</p>
<p>LightGBM 是一种高效的梯度提升决策树实现，旨在提高 GBDT
的训练速度和预测精度。</p>
<p><strong>LightGBM 的关键特性和数学原理</strong>：</p>
<ol type="1">
<li><strong>基于叶子节点的增长策略（Leaf-wise Growth）</strong></li>
<li><strong>基于直方图的算法（Histogram-based Algorithm）</strong></li>
<li><strong>梯度单边采样（Gradient-based One-Side Sampling,
GOSS）</strong></li>
<li><strong>特征并行和数据并行（Feature and Data
Parallelism）</strong></li>
</ol>
<p><br></p>
<h5 id="基于叶子节点的增长策略leaf-wise-growth">基于叶子节点的增长策略（Leaf-wise
Growth）</h5>
<p>LightGBM 使用 leaf-wise（基于叶子节点的增长策略）而不是
level-wise（基于层的增长策略），每次选择具有最大增益的叶子节点进行分裂。</p>
<p><span id="zengyi"></span></p>
<ul>
<li><p><strong>增益计算</strong>：</p>
<p><span class="math display">\[
\text{Gain} = \frac{1}{2} \left( \frac{G_L^2}{H_L + \lambda} +
\frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda}
\right) - \gamma
\]</span></p>
<p>其中：</p>
<ul>
<li>$ G_L $ 和 $ G_R $ 分别是左、右子节点的梯度和</li>
<li>$ H_L $ 和 $ H_R $ 分别是左、右子节点的二阶梯度和</li>
<li><span class="math inline">\(\lambda\)</span> 是 L2 正则化参数</li>
<li><span class="math inline">\(\gamma\)</span>
是叶子节点分裂的惩罚项</li>
<li><strong>左子节点梯度和</strong>: <span class="math inline">\(G_L=\sum_{x_i \leq \text { split }}
g_i\)</span></li>
<li><strong>右子节点梯度和</strong>： <span class="math inline">\(G_R=\sum_{x_i&gt;\text { split }}
g_i\)</span></li>
<li><strong>左子节点二阶梯度和</strong>: <span class="math inline">\(H_L=\sum_{x_i \leq \text { split }}
h_i\)</span></li>
<li><strong>右子节点二阶梯度和</strong>: <span class="math inline">\(H_R=\sum_{x_i&gt;\text { split }}
h_i\)</span></li>
</ul></li>
</ul>
<hr>
<h5 id="基于直方图的算法histogram-based-algorithm">基于直方图的算法（Histogram-based
Algorithm）</h5>
<p>LightGBM 使用直方图算法将连续特征值离散化为有限数量的
<em>bin</em>，大大减少了计算量。每次分裂节点时，LightGBM
会计算所有特征的直方图，然后选择最佳分裂点。</p>
<ul>
<li><p><strong>直方图构建以及直方图加速分裂原理</strong>：</p>
<ol type="1">
<li><p>离散化特征值：</p>
<p>将连续特征值划分为 <span class="math inline">\(k\)</span> 个
bin，每个 bin 表示一个区间。</p>
<ul>
<li>离散化 (分箱)：将连续特征值 <span class="math inline">\(x_i\)</span>
转换为离散的 bin 索引 <span class="math inline">\(b_j\)</span> 。</li>
<li>公式: 假设特征值 <span class="math inline">\(X\)</span> 的范围是
<span class="math inline">\([a, b]\)</span> ，划分为 <span class="math inline">\(k\)</span> 个 bin，每个 bin 的宽度为 <span class="math inline">\(\Delta=\frac{b-a}{k}\)</span> <span class="math display">\[
\operatorname{bin}_j=[a+(j-1) \Delta, a+j \Delta)
\]</span></li>
<li>分配样本到 bin: 对于每个样本 <span class="math inline">\(x_i\)</span> ，确定其所属的 bin 索引 <span class="math inline">\(j\)</span> : <span class="math display">\[
j=\left\lfloor\frac{x_i-a}{\Delta}\right\rfloor
\]</span></li>
</ul></li>
<li><p>统计每个 bin 中的样本数量和梯度：</p>
<p>对于每个 $ bin_j $ ，计算其包含的样本数量 <span class="math inline">\(N_j\)</span> 、样本的一阶梯度和 <span class="math inline">\(G_j\)</span> 以及二阶梯度和 <span class="math inline">\(H_j\)</span> 。</p>
<ul>
<li><p>样本数量统计： <span class="math display">\[
N_j = \sum_{i=1}^{n} \mathbb{I}(x_i \in \text{bin}_j)
\]</span> 其中，</p>
<ul>
<li><p>$ N_j $ 表示 $ bin_j $ 中样本的数量</p></li>
<li><p><span class="math inline">\(\mathbb{I}\)</span>
是指示函数，表示样本 $ x_i $ 是否属于 $ bin_j $。即： <span class="math display">\[
   \mathbb{I}\left(x_i \in \operatorname{bin}_j\right)= \begin{cases}1,
&amp; \text { 如果 } x_i \in \operatorname{bin}_j \\ 0, &amp; \text {
如果 } x_i \notin \operatorname{bin}_j\end{cases}
   \]</span></p></li>
</ul></li>
<li><p>梯度和统计： <span class="math display">\[
\begin{aligned}G_j &amp; =\sum_{i=1}^n \mathbb{I}\left(x_i \in
\operatorname{bin}_j\right) g_i \\H_j &amp; =\sum_{i=1}^n
\mathbb{I}\left(x_i \in \operatorname{bin}_j\right) h_i\end{aligned}
\]</span> 其中，</p>
<ul>
<li><span class="math inline">\(G_j\)</span> 表示 $ bin_j $
的一阶梯度和，<span class="math inline">\(g_i\)</span> 是样本 <span class="math inline">\(x_i\)</span> 的一阶梯度</li>
<li><span class="math inline">\(H_j\)</span> 表示 $ bin_j $
的二阶梯度和，<span class="math inline">\(h_i\)</span> 是样本 <span class="math inline">\(x_i\)</span> 的二阶梯度</li>
</ul></li>
</ul></li>
<li><p>计算每个分裂点的增益<br>在计算增益时，我们需要遍历所有可能的分裂点，计算每个分裂点的<a href="#zengyi">增益</a>。</p></li>
<li><p>遍历所有可能的分裂点，计算增益<br>我们需要遍历所有可能的分裂点，计算每个分裂点的增益，并选择增益最大的分裂点作为最佳分裂点。</p>
<ol type="1">
<li><p>初始化增益和最佳分裂点：<br> best_gain <span class="math inline">\(=-\infty\)</span><br> best_split <span class="math inline">\(=\)</span> None</p></li>
<li><p>遍历所有分裂点：</p>
<p>对于每个可能的分裂点，计算其增益，并更新最佳增益和最佳分裂点。 for
each split point <span class="math inline">\(s_j\)</span> : <span class="math display">\[
\begin{aligned}
for\ &amp; each\ split\ point\ s_j:\\
    &amp; G_L  =\sum_{x_i \leq s_j} g_i \\
    &amp; H_L  =\sum_{x_i \leq s_j} h_i \\
    &amp; G_R  =\sum_{x_i&gt;s_j} g_i \\
    &amp; H_R  =\sum_{x_i&gt;s_j} h_i
\end{aligned}
\]</span></p>
<p>计算增益并更新最佳增益和最佳分裂点： <span class="math display">\[
\begin{aligned}
if\ G &amp; ain\_j\ &gt;\ best\_gain\ :\\
    &amp; best\_gain\ =\ Gain_j\\
    &amp; best\_split\ =\ s_j
\end{aligned}
\]</span></p></li>
</ol></li>
<li><p>选择最佳分裂点<br>最终，选择具有最大增益的分裂点作为最佳分裂点。<br>最佳分裂点
= best_split<br></p></li>
</ol>
<pre><code>最佳增益 = best_gain</code></pre></li>
</ul>
<h5 id="梯度单边采样goss">梯度单边采样（GOSS）</h5>
<p>GOSS
通过保持大梯度的数据点，随机采样小梯度的数据点来减少计算量，同时保持模型的准确性。</p>
<ul>
<li><p><strong>大梯度样本保留</strong>：<br>
保留那些具有较大梯度的样本，因为这些样本对提升的损失函数贡献最大。</p></li>
<li><p><strong>小梯度样本采样</strong>：<br>
随机采样那些具有较小梯度的样本，减少计算量，并通过对小梯度样本赋予更大的权重来平衡样本分布。</p></li>
</ul>
<p><strong>GOSS 的步骤</strong></p>
<p><img src="../../../../images/dianli/fig3.png"> &gt;GOSS
伪代码，引自<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_24519677/article/details/82811215">Lightgbm基本原理介绍</a></p>
<p>假设我们有一个样本 <span class="math inline">\(\left\{\left(x_i,
y_i\right)\right\}_{i=1}^n\)</span> ，其梯度为 <span class="math inline">\(\left\{g_i\right\}_{i=1}^n\)</span> 。</p>
<p>以下是 GOSS 的具体步骤:</p>
<ol type="1">
<li><p>计算每个样本的梯度:</p>
<ul>
<li>对于每个样本 <span class="math inline">\(i\)</span> ，计算其梯度
<span class="math inline">\(g_i\)</span> : <span class="math display">\[
g_i=\frac{\partial L\left(y_i, \hat{y}_i\right)}{\partial \hat{y}_i}
\]</span> 其中， <span class="math inline">\(L\)</span> 是损失函数，
<span class="math inline">\(y_i\)</span> 是实际值， <span class="math inline">\(\hat{y}_i\)</span> 是预测值。</li>
</ul></li>
<li><p>排序样本梯度:</p>
<ul>
<li>将样本按梯度的绝对值 <span class="math inline">\(\left|g_i\right|\)</span> 从大到小排序。</li>
</ul></li>
<li><p>选择大梯度样本:</p>
<ul>
<li>选择前 <span class="math inline">\(a \cdot n\)</span>
个梯度较大的样本，其中 <span class="math inline">\(a\)</span>
是一个比例参数。</li>
</ul></li>
<li><p>随机采样小梯度样本:</p>
<ul>
<li>从剩余的 <span class="math inline">\((1-a) \cdot n\)</span>
个样本中随机选择 <span class="math inline">\(b \cdot n\)</span>
个梯度较小的样本，其中 <span class="math inline">\(b\)</span>
也是一个比例参数。</li>
</ul></li>
<li><p>调整小梯度样本的权重:</p>
<ul>
<li>为了平衡样本分布，对小梯度样本的梯度进行放大，放大系数为 <span class="math inline">\(\frac{1-a}{b}\)</span> 。</li>
</ul></li>
</ol>
<h5 id="特征并行和数据并行">特征并行和数据并行</h5>
<p>LightGBM
支持特征并行和数据并行，通过多线程和分布式计算进一步加速训练过程。</p>
<h3 id="训练和预测并保存结果">训练和预测并保存结果</h3>
<p>使用 time_model 函数训练模型，并进行验证和测试集预测。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">train_cv = train</span><br><span class="line">test_cv = test</span><br><span class="line">train_cols_cv = train_cols</span><br><span class="line">lgb_oof, lgb_test = time_model(lgb, train, test, train_cols)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>184.6440690468036</code></pre>
<p>将测试集预测结果保存到 submit.csv 文件中。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">test[<span class="string">'target'</span>] = lgb_test</span><br><span class="line">test[[<span class="string">'id'</span>,<span class="string">'dt'</span>,<span class="string">'target'</span>]].to_csv(<span class="string">'submit.csv'</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>提交结果，可以看到得分为：259.9667</p>
<p>相对于 Task 1 有较大的提升。</p>
<h3 id="超参数优化">超参数优化</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">time_model_gc</span>(<span class="params">train_df, test_df, cols</span>):</span><br><span class="line">    <span class="comment"># 训练集和验证集切分</span></span><br><span class="line">    trn_x, trn_y = train_df[train_df.dt &gt;= <span class="number">31</span>][cols], train_df[train_df.dt &gt;= <span class="number">31</span>][<span class="string">'target'</span>]</span><br><span class="line">    val_x, val_y = train_df[train_df.dt &lt;= <span class="number">30</span>][cols], train_df[train_df.dt &lt;= <span class="number">30</span>][<span class="string">'target'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建模型输入数据</span></span><br><span class="line">    train_matrix = lgb.Dataset(trn_x, label=trn_y)</span><br><span class="line">    valid_matrix = lgb.Dataset(val_x, label=val_y, reference=train_matrix)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义参数网格</span></span><br><span class="line">    param_grid = {</span><br><span class="line">        <span class="string">'learning_rate'</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>],</span><br><span class="line">        <span class="string">'num_leaves'</span>: [<span class="number">16</span>, <span class="number">32</span>],</span><br><span class="line">        <span class="string">'max_depth'</span>: [<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">        <span class="string">'min_child_weight'</span>: [<span class="number">5</span>, <span class="number">10</span>],</span><br><span class="line">        <span class="string">'subsample'</span>: [<span class="number">0.8</span>, <span class="number">1.0</span>],</span><br><span class="line">        <span class="string">'reg_lambda'</span>: [<span class="number">0.1</span>, <span class="number">0.5</span>],</span><br><span class="line">        <span class="string">'colsample_bytree'</span>: [<span class="number">0.8</span>, <span class="number">1</span>]</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化LGBMRegressor，使用GPU</span></span><br><span class="line">    lgb_model = LGBMRegressor(</span><br><span class="line">        boosting_type=<span class="string">'gbdt'</span>,</span><br><span class="line">        objective=<span class="string">'regression'</span>,</span><br><span class="line">        metric=<span class="string">'mse'</span>,</span><br><span class="line">       <span class="comment"># min_child_weight=5,</span></span><br><span class="line">       <span class="comment"># num_leaves=2 ** 5,</span></span><br><span class="line">        reg_lambda=<span class="number">10</span>,</span><br><span class="line">       <span class="comment"># colsample_bytree=0.8,</span></span><br><span class="line">       <span class="comment"># subsample=0.8,</span></span><br><span class="line">        subsample_freq=<span class="number">4</span>,</span><br><span class="line">        learning_rate=<span class="number">0.05</span>,</span><br><span class="line">        seed=<span class="number">666</span>,</span><br><span class="line">        n_jobs=<span class="number">16</span>,</span><br><span class="line">        verbose=<span class="number">0</span>,</span><br><span class="line">        device=<span class="string">'gpu'</span>,</span><br><span class="line">        gpu_platform_id=<span class="number">0</span>,</span><br><span class="line">        gpu_device_id=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行网格搜索</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"开始网格搜索..."</span>)</span><br><span class="line">    grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, scoring=<span class="string">'neg_mean_squared_error'</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span>)</span><br><span class="line">    grid_search.fit(trn_x, trn_y)</span><br><span class="line">    best_params = grid_search.best_params_</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"最佳参数: <span class="subst">{best_params}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"最佳评分: <span class="subst">{-grid_search.best_score_}</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用最佳参数重新训练模型</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"开始重新训练模型..."</span>)</span><br><span class="line">    lgb_model = lgb.train(</span><br><span class="line">        params=best_params,</span><br><span class="line">        train_set=train_matrix,</span><br><span class="line">        valid_sets=[train_matrix, valid_matrix],</span><br><span class="line">        num_boost_round=<span class="number">5000</span>,</span><br><span class="line">        early_stopping_rounds=<span class="number">500</span>,</span><br><span class="line">        verbose_eval=<span class="number">200</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 验证集预测</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"开始验证集预测..."</span>)</span><br><span class="line">    val_pred = lgb_model.predict(val_x, num_iteration=lgb_model.best_iteration)</span><br><span class="line">    mse = mean_squared_error(val_y, val_pred)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'Validation Mean Squared Error: <span class="subst">{mse}</span>'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试集预测</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"开始测试集预测..."</span>)</span><br><span class="line">    test_pred = lgb_model.predict(test_df[cols], num_iteration=lgb_model.best_iteration)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> val_pred, test_pred, val_x.index</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">train = train_cv</span><br><span class="line">test = test_cv</span><br><span class="line">train_cols = train_cols_cv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 time_model_gc 函数并接收结果</span></span><br><span class="line">val_pred, test_pred, val_index= time_model_gc(train, test, train_cols)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>开始网格搜索...
Fitting 5 folds for each of 1296 candidates, totalling 6480 fits</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 保存验证集预测结果</span></span><br><span class="line">val_results = pd.DataFrame({</span><br><span class="line">    <span class="string">'id'</span>: train[<span class="string">'id'</span>][train[<span class="string">'dt'</span>] &lt;= <span class="number">30</span>],</span><br><span class="line">    <span class="string">'dt'</span>: train[<span class="string">'dt'</span>][train[<span class="string">'dt'</span>] &lt;= <span class="number">30</span>],</span><br><span class="line">    <span class="string">'actual'</span>: train[<span class="string">'target'</span>][train[<span class="string">'dt'</span>] &lt;= <span class="number">30</span>],</span><br><span class="line">    <span class="string">'predicted'</span>: val_pred</span><br><span class="line">})</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存验证集预测结果</span></span><br><span class="line">val_results = pd.DataFrame({</span><br><span class="line">    <span class="string">'id'</span>: train.loc[val_index, <span class="string">'id'</span>],</span><br><span class="line">    <span class="string">'dt'</span>: train.loc[val_index, <span class="string">'dt'</span>],</span><br><span class="line">    <span class="string">'type'</span>: train.loc[val_index, <span class="string">'type'</span>],</span><br><span class="line">    <span class="string">'target'</span>: val_pred</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">val_results.to_csv(<span class="string">'validation_predictions.csv'</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存测试集预测结果</span></span><br><span class="line">test_results = pd.DataFrame({</span><br><span class="line">    <span class="string">'id'</span>: test[<span class="string">'id'</span>],</span><br><span class="line">    <span class="string">'dt'</span>: test[<span class="string">'dt'</span>],</span><br><span class="line">    <span class="string">'type'</span>: test[<span class="string">'type'</span>],</span><br><span class="line">    <span class="string">'target'</span>: test_pred</span><br><span class="line">})</span><br><span class="line"></span><br><span class="line">test_results.to_csv(<span class="string">'test_predictions.csv'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://studyincau.github.io">小楼一夜听春雨 &amp; Rico</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://studyincau.github.io/2024/07/19/dian-li-yu-ce/">https://studyincau.github.io/2024/07/19/dian-li-yu-ce/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://studyincau.github.io" target="_blank">StudyinCAU</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/../images/xunfei.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/22/pylearn/" title="Python基本语法"><img class="cover" src="/../images/python3.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Python基本语法</div></div></a></div><div class="next-post pull-right"><a href="/2024/07/17/ji-yu-zhu-yu-ci-dian-gan-yu-de-ji-qi-fan-yi-tiao-zhan-sai/" title="基于术语词典干预的机器翻译挑战赛"><img class="cover" src="/../images/xunfei.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">基于术语词典干预的机器翻译挑战赛</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/05/28/ji-qi-xue-xi-gai-lun/" title="机器学习概论"><img class="cover" src="/../images/ML.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-28</div><div class="title">机器学习概论</div></div></a></div><div><a href="/2024/04/28/di-01-zhang-tong-ji-xue-xi-fang-fa-gai-lun/" title="第1章 统计学习方法概论"><img class="cover" src="/../images/ML.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-28</div><div class="title">第1章 统计学习方法概论</div></div></a></div><div><a href="/2024/04/28/di-02-zhang-gan-zhi-ji/" title="第2章 感知机"><img class="cover" src="/../images/ML.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-28</div><div class="title">第2章 感知机</div></div></a></div><div><a href="/2024/04/29/di-03-zhang-k-jin-lin-fa/" title="第3章 k近邻法"><img class="cover" src="/../images/ML.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-29</div><div class="title">第3章 k近邻法</div></div></a></div><div><a href="/2024/04/29/di-4-zhang-po-su-bei-xie-si/" title="第4章 朴素贝叶斯"><img class="cover" src="/../images/ML.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-29</div><div class="title">第4章 朴素贝叶斯</div></div></a></div><div><a href="/2024/07/17/ji-yu-zhu-yu-ci-dian-gan-yu-de-ji-qi-fan-yi-tiao-zhan-sai/" title="基于术语词典干预的机器翻译挑战赛"><img class="cover" src="/../images/xunfei.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-17</div><div class="title">基于术语词典干预的机器翻译挑战赛</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">小楼一夜听春雨 &amp; Rico</div><div class="author-info__description">Study</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">120</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">212</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">56</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/StudyinCAU"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/StudyinCAU" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:3187248635@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这是一个分享学习笔记(资料)的网站，欢迎一起学习、交流。如果你有好的文章也想分享，可以发邮箱(邮箱在公告上方~)[若出现渲染问题或加载过慢，推荐用谷歌或Edge浏览器打开]</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A2%98%E7%9B%AE"><span class="toc-text">题目</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B5%9B%E9%A2%98%E4%BB%BB%E5%8A%A1"><span class="toc-text">赛题任务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E5%AE%A1%E8%A7%84%E5%88%99"><span class="toc-text">评审规则</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%B4%E6%98%8E"><span class="toc-text">数据说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E5%AE%A1%E8%A7%84%E5%88%99-1"><span class="toc-text">评审规则</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%B8%8B%E8%BD%BD"><span class="toc-text">数据下载</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#task-1"><span class="toc-text">Task 1</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#task-2"><span class="toc-text">Task 2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="toc-text">基础概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lightgbm"><span class="toc-text">LightGBM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5%E6%A8%A1%E5%9D%97"><span class="toc-text">导入模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-text">读取数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">数据可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9F%B1%E7%8A%B6%E5%9B%BE"><span class="toc-text">柱状图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8A%98%E7%BA%BF%E5%9B%BE"><span class="toc-text">折线图</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B9%B3%E6%BB%91%E5%A4%84%E7%90%86%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-text">平滑处理的原理</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="toc-text">数学原理</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E5%92%8C%E6%8E%92%E5%BA%8F%E6%95%B0%E6%8D%AE"><span class="toc-text">合并和排序数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%86%E5%8F%B2%E5%B9%B3%E7%A7%BB"><span class="toc-text">历史平移</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AA%97%E5%8F%A3%E7%BB%9F%E8%AE%A1"><span class="toc-text">窗口统计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%88%86%E5%89%B2"><span class="toc-text">数据分割</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A1%AE%E5%AE%9A%E8%BE%93%E5%85%A5%E7%89%B9%E5%BE%81"><span class="toc-text">确定输入特征</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0"><span class="toc-text">定义模型训练函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-text">代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3"><span class="toc-text">参数详解：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-text">
LightGBM
</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9%E7%9A%84%E5%A2%9E%E9%95%BF%E7%AD%96%E7%95%A5leaf-wise-growth"><span class="toc-text">基于叶子节点的增长策略（Leaf-wise
Growth）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%9B%B4%E6%96%B9%E5%9B%BE%E7%9A%84%E7%AE%97%E6%B3%95histogram-based-algorithm"><span class="toc-text">基于直方图的算法（Histogram-based
Algorithm）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E5%8D%95%E8%BE%B9%E9%87%87%E6%A0%B7goss"><span class="toc-text">梯度单边采样（GOSS）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%B9%B6%E8%A1%8C%E5%92%8C%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="toc-text">特征并行和数据并行</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E9%A2%84%E6%B5%8B%E5%B9%B6%E4%BF%9D%E5%AD%98%E7%BB%93%E6%9E%9C"><span class="toc-text">训练和预测并保存结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96"><span class="toc-text">超参数优化</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/07/22/pylearn/" title="Python基本语法"><img src="/../images/python3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python基本语法"/></a><div class="content"><a class="title" href="/2024/07/22/pylearn/" title="Python基本语法">Python基本语法</a><time datetime="2024-07-22T12:43:35.000Z" title="发表于 2024-07-22 20:43:35">2024-07-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/19/dian-li-yu-ce/" title="电力需求预测赛"><img src="/../images/xunfei.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="电力需求预测赛"/></a><div class="content"><a class="title" href="/2024/07/19/dian-li-yu-ce/" title="电力需求预测赛">电力需求预测赛</a><time datetime="2024-07-19T15:33:35.000Z" title="发表于 2024-07-19 23:33:35">2024-07-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/17/ji-yu-zhu-yu-ci-dian-gan-yu-de-ji-qi-fan-yi-tiao-zhan-sai/" title="基于术语词典干预的机器翻译挑战赛"><img src="/../images/xunfei.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="基于术语词典干预的机器翻译挑战赛"/></a><div class="content"><a class="title" href="/2024/07/17/ji-yu-zhu-yu-ci-dian-gan-yu-de-ji-qi-fan-yi-tiao-zhan-sai/" title="基于术语词典干预的机器翻译挑战赛">基于术语词典干预的机器翻译挑战赛</a><time datetime="2024-07-17T13:55:57.000Z" title="发表于 2024-07-17 21:55:57">2024-07-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/27/shu-li-tong-ji-6-5-qu-jian-gu-ji/" title="第五节 区间估计"><img src="/../images/background46.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第五节 区间估计"/></a><div class="content"><a class="title" href="/2024/06/27/shu-li-tong-ji-6-5-qu-jian-gu-ji/" title="第五节 区间估计">第五节 区间估计</a><time datetime="2024-06-27T08:15:37.000Z" title="发表于 2024-06-27 16:15:37">2024-06-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/01/hua-tu/" title="Python可视化"><img src="/../images/seaborn.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python可视化"/></a><div class="content"><a class="title" href="/2024/06/01/hua-tu/" title="Python可视化">Python可视化</a><time datetime="2024-05-31T18:33:05.000Z" title="发表于 2024-06-01 02:33:05">2024-06-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By 小楼一夜听春雨 & Rico</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'xFi1vMHzzqw1qFKQbyK4WtAo-gzGzoHsz',
      appKey: 'hGyKCk4AFAr9aPSCHioyb0hH',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !true) {
    if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><link rel="stylesheet" href="/css/title.css"><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>