<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习概论 | StudyinCAU</title><meta name="author" content="小楼一夜听春雨 &amp; Rico"><meta name="copyright" content="小楼一夜听春雨 &amp; Rico"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习概论 本文章将把机器学习大致内容梳理一遍，可以快速了解机器学习的大致框架及大致内容。 一、线性回归 1.线性回归模型的基本原理和假设 线性回归是⼀种⼴泛⽤于统计学和机器学习中的回归分析⽅法，⽤于建⽴⾃变量（特征）与因变量（⽬标）之间的 线性关系模型。线性回归的基本原理是寻找⼀条直线（或者在多维情况下是⼀个超平⾯），以最佳地拟合训练数 据，使得模型的预测与真实观测值之间的误差最⼩化。下⾯我们">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习概论">
<meta property="og:url" content="https://studyincau.github.io/2024/05/28/ji-qi-xue-xi-gai-lun/index.html">
<meta property="og:site_name" content="StudyinCAU">
<meta property="og:description" content="机器学习概论 本文章将把机器学习大致内容梳理一遍，可以快速了解机器学习的大致框架及大致内容。 一、线性回归 1.线性回归模型的基本原理和假设 线性回归是⼀种⼴泛⽤于统计学和机器学习中的回归分析⽅法，⽤于建⽴⾃变量（特征）与因变量（⽬标）之间的 线性关系模型。线性回归的基本原理是寻找⼀条直线（或者在多维情况下是⼀个超平⾯），以最佳地拟合训练数 据，使得模型的预测与真实观测值之间的误差最⼩化。下⾯我们">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://studyincau.github.io/images/ML.jpg">
<meta property="article:published_time" content="2024-05-28T15:07:22.000Z">
<meta property="article:modified_time" content="2024-05-28T15:47:13.000Z">
<meta property="article:author" content="小楼一夜听春雨 &amp; Rico">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://studyincau.github.io/images/ML.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://studyincau.github.io/2024/05/28/ji-qi-xue-xi-gai-lun/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习概论',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-05-28 23:47:13'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.1.1">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">124</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">207</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">53</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/../images/ML.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="StudyinCAU"><span class="site-name">StudyinCAU</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习概论</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-05-28T15:07:22.000Z" title="发表于 2024-05-28 23:07:22">2024-05-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-28T15:47:13.000Z" title="更新于 2024-05-28 23:47:13">2024-05-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/">机器学习概论</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>33分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习概论"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="机器学习概论">机器学习概论</h1>
<p>本文章将把机器学习大致内容梳理一遍，可以快速了解机器学习的大致框架及大致内容。</p>
<h1 id="一线性回归">一、线性回归</h1>
<h2 id="线性回归模型的基本原理和假设">1.线性回归模型的基本原理和假设</h2>
<p>线性回归是⼀种⼴泛⽤于统计学和机器学习中的回归分析⽅法，⽤于建⽴⾃变量（特征）与因变量（⽬标）之间的
线性关系模型。线性回归的基本原理是寻找⼀条直线（或者在多维情况下是⼀个超平⾯），以最佳地拟合训练数
据，使得模型的预测与真实观测值之间的误差最⼩化。下⾯我们来详细解释线性回归的基本原理和假设。</p>
<p><strong>简单线性回归模型</strong>： <span class="math display">\[
y=b_0+b_1\cdot x
\]</span> <strong>多元线性回归模型</strong>： <span class="math display">\[
y=b_0+b_1\cdot x_1+b_2\cdot x_2+\ldots+b_p\cdot x_p
\]</span></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(y\)</span>是因变量 (需要预测的值)。</li>
<li><span class="math inline">\(x_1, x_2, \ldots , x_p\)</span>是自变量
(特征),可以是一个或多个。</li>
<li><span class="math inline">\(b_0\)</span>是截距(模型在自变量都为0时的预测值)。</li>
<li><span class="math inline">\(b_1,b_2,\ldots,b_p\)</span>是回归系数，表示自变量对因变量的影响程度。</li>
</ul>
<p><strong>最优解</strong></p>
<ul>
<li><code>Actual value</code>: 真实值,即已知的</li>
<li><code>Predicted value</code>:预测值，是把已知的<code>x</code>带入到公式里面和猜出来的参数
<code>a,b</code> 计算得到的</li>
<li><code>Error</code>:误差，预测值和真实值的差距</li>
<li><code>最优解</code>：尽可能的找到一个模型使得整体的误差最小，整体的误差通常叫做损失
<code>Loss</code></li>
<li><code>Loss</code>:整体的误差，<code>loss</code> 通过损失函数
<code>Ioss function</code> 计算得到</li>
</ul>
<p>线性回归的目标是找到合适的回归系数<span class="math inline">\(b_0,b_1,\ldots,b_p\)</span>,以最小化模型的预测误差。通常采用最小二乘法来估计这
些系数，即使得观测值与模型预测值之间的残差平方和最小。</p>
<p>线性回归模型的有效性基于以下⼀些关键假设：</p>
<ol type="1">
<li><strong>线性关系假设</strong>：线性回归假设因变量和自变量之间存在线性关系。这意味着模型试图用一条直线(或超平面)来拟合数据，以描述自变量与因变量之间的关系。</li>
<li><strong>独立性假设</strong>：线性回归假设每个观测值之间是相互独立的。这意味看一个观测值的误差不受其他观测值的影响。</li>
<li><strong>常数方差假设</strong>：线性回归假设在自变量的每个取值点上，观测值的误差方差都是常数。这被称为同方差性或等方差性。</li>
<li><strong>正态性假设</strong>：线性回归假设观测值的误差服从正态分布。这意味着在不同自变量取值点上的误差应该接近正态分布。</li>
</ol>
<p>如果这些假设不满⾜，线性回归模型的结果可能不可靠。</p>
<p><strong>⼀个简单示例</strong>：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(y\)</span></th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">8</th>
<th style="text-align: center;">7</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(x\)</span></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">7</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.font_manager <span class="keyword">as</span> fm</span><br><span class="line">font_path = <span class="string">'C:\\Windows\\Fonts\\simsun.ttc'</span>  </span><br><span class="line">font_prop = fm.FontProperties(fname=font_path)</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimSun'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建示例数据</span></span><br><span class="line">X = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 自变量</span></span><br><span class="line">y = np.array([<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">7</span>])  <span class="comment"># 因变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合模型</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印回归系数和截距</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"回归系数 (b1):"</span>, model.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"截距 (b0):"</span>, model.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测新数据点</span></span><br><span class="line">new_x = np.array([<span class="number">6</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">predicted_y = model.predict(new_x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"新数据点的预测值:"</span>, predicted_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制数据点和拟合结果</span></span><br><span class="line">plt.scatter(X, y, color=<span class="string">'blue'</span>, label=<span class="string">'实际值'</span>)</span><br><span class="line">plt.plot(X, model.predict(X), color=<span class="string">'red'</span>, linewidth=<span class="number">2</span>, label=<span class="string">'拟合线'</span>)</span><br><span class="line">plt.scatter(new_x, predicted_y, color=<span class="string">'green'</span>, marker=<span class="string">'x'</span>, s=<span class="number">100</span>, label=<span class="string">'新数据点'</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'线性回归示例'</span>, fontproperties=font_prop)</span><br><span class="line">plt.xlabel(<span class="string">'自变量 (X)'</span>, fontproperties=font_prop)</span><br><span class="line">plt.ylabel(<span class="string">'因变量 (y)'</span>, fontproperties=font_prop)</span><br><span class="line">plt.legend(prop=font_prop)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<pre><code>回归系数 (b1): [0.82142857]
截距 (b0): 1.7142857142857149
新数据点的预测值: [6.64285714]</code></pre>
<p><img src="../../../../images/机器学习概论/output_1_1.png"></p>
<h2 id="参数估计">2.参数估计</h2>
<p>使用最小二乘法求解线性回归模型的参数通常包括以下步骤：</p>
<ul>
<li>定义模型：首先，需要定义线性回归模型的数学形式，即<span class="math display">\[Y=\beta_0+\beta_1X_1+\beta_2X_2+\ldots+\beta_nX_n+\epsilon\]</span>其
中<span class="math inline">\(Y\)</span>是因变量，<span class="math inline">\(X_1,X_2,\ldots,X_n\)</span>是自变量，<span class="math inline">\(\beta_0,\beta_1,\ldots,\beta_n\)</span>是模型的参数，<span class="math inline">\(\epsilon\)</span>是误差项。</li>
<li>确定损失函数：损失函数是用来衡量模型预测值与实际观测值之间的差异的函数。在最小二乘法中，通常使
用残差平方和作为损失函数。</li>
<li>最小化损失函数：使用优化算法
(通常是梯度下降法或闭式解)来最小化损失函数。在最小二乘法中，要找
到使得残差平方和最小的参数值。对于线性回归模型，可以通过求解以下正规方程来得</li>
</ul>
<p>到闭式解： <span class="math display">\[
\beta=(X^TX)^{-1}X^TY
\]</span> 其中<span class="math inline">\(X\)</span>是自变量的设计矩阵，<span class="math inline">\(Y\)</span> 是因变量的观测值，<span class="math inline">\(\beta\)</span> 是参数向量。</p>
<h3 id="最小二乘法的原理">最小二乘法的原理</h3>
<p>在线性回归中, 我们假设自变量 <span class="math inline">\(x\)</span>
和因变量 <span class="math inline">\(y\)</span> 之间存在线性关系,
可以用以下模型表示： <span class="math display">\[
y=\beta_0+\beta_1 \cdot x+\varepsilon
\]</span></p>
<p>其中: - <span class="math inline">\(y\)</span>
是因变量（需要预测的值）。 - <span class="math inline">\(x\)</span>
是自变量（特征）。 - <span class="math inline">\(\beta_0\)</span>
是截距（模型在自变量为 0 时的预测值）。 - <span class="math inline">\(\beta_1\)</span> 是斜率（自变量 <span class="math inline">\(x\)</span> 对因变量 <span class="math inline">\(y\)</span> 的影响程度）。 - <span class="math inline">\(\varepsilon\)</span> 是误差项,
表示模型无法完美拟合真实数据的部分。</p>
<p>最小二乘法的目标是找到最佳的 <span class="math inline">\(\beta_0\)</span> 和 <span class="math inline">\(\beta_1\)</span>, 使得观测值 <span class="math inline">\(y_i\)</span> 和对应的模型预测值 <span class="math inline">\(\hat{y}_i\)</span>
之间的残差（差值）的平方和最小化: <span class="math display">\[
\min _{\beta_0, \beta_1} \sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2
\]</span> 其中, <span class="math inline">\(n\)</span> 是样本数量, <span class="math inline">\(\hat{y}_i\)</span> 是根据模型预测的值。
<strong>最小二乘法的公式</strong></p>
<p>最小二乘法可以通过以下公式来估计参数 <span class="math inline">\(\beta_0\)</span> 和 <span class="math inline">\(\beta_1\)</span> : <span class="math display">\[
\begin{aligned}
\beta_1 &amp;=
\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}
\\
\beta_0 &amp;= \bar{y}-\beta_1 \cdot \bar{x}
\end{aligned}
\]</span></p>
<p>其中, <span class="math inline">\(\bar{x}\)</span> 和 <span class="math inline">\(\bar{y}\)</span> 分别是自变量 <span class="math inline">\(x\)</span> 和因变量 <span class="math inline">\(y\)</span> 的均值。</p>
<p><strong>最⼩⼆乘法的代码示例</strong>
使用常见的线性回归数据集加州房价数据集。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.font_manager <span class="keyword">as</span> fm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载加州房价数据集</span></span><br><span class="line">california = fetch_california_housing()</span><br><span class="line">X = california.data[:, [<span class="number">0</span>]]  <span class="comment"># 使用第一个特征（房间数均值）作为自变量</span></span><br><span class="line">y = california.target        <span class="comment"># 房价中位数作为因变量</span></span><br><span class="line"></span><br><span class="line">X = X[:<span class="number">200</span>]</span><br><span class="line">y = y[:<span class="number">200</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合模型</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取回归系数和截距</span></span><br><span class="line">beta_1 = model.coef_[<span class="number">0</span>]</span><br><span class="line">beta_0 = model.intercept_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印回归系数和截距</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"斜率 (beta_1):"</span>, beta_1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"截距 (beta_0):"</span>, beta_0)</span><br><span class="line"></span><br><span class="line">font_path = <span class="string">'C:\\Windows\\Fonts\\simsun.ttc'</span></span><br><span class="line">font_prop = fm.FontProperties(fname=font_path)</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimSun'</span>]</span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span>  <span class="comment"># 解决负号显示问题</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制数据点和拟合结果</span></span><br><span class="line">plt.scatter(X, y, color=<span class="string">'blue'</span>, label=<span class="string">'实际值'</span>)</span><br><span class="line">plt.plot(X, model.predict(X), color=<span class="string">'red'</span>, linewidth=<span class="number">2</span>, label=<span class="string">'拟合线'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加标题和标签</span></span><br><span class="line">plt.title(<span class="string">'线性回归示例'</span>, fontproperties=font_prop)</span><br><span class="line">plt.xlabel(<span class="string">'房间数均值'</span>, fontproperties=font_prop)</span><br><span class="line">plt.ylabel(<span class="string">'房价中位数'</span>, fontproperties=font_prop)</span><br><span class="line">plt.legend(prop=font_prop)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图形</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<pre><code>斜率 (beta_1): 0.3849371126330091
截距 (beta_0): 0.7464519437859625</code></pre>
<p><img src="../../../../images/机器学习概论/output_4_1.png"></p>
<p>代码中, 使用<code>Scikit-Learn</code>的 <code>LinearRegression</code>
类来创建并拟合线性回归模型。 最小二乘法的原理被内部自动应用,
从而估计出最佳的回归系数 <span class="math inline">\(\beta_0\)</span> 和
<span class="math inline">\(\beta_1\)</span>
。这些系数将用于构建最佳拟合线性模型,
以最小化观测值和模型预测值之间的残差平方和。</p>
<h2 id="评估指标">3.评估指标</h2>
<p>在进⾏参数估计后，通常需要对估计结果进⾏评价，以确保模型的可靠性和有效性。</p>
<p>评价参数估计的常用方法包括: -
残差分析：分析残差的分布和模式，检验模型的拟合效果和误差项的假设是否成立。
- 参数显著性检验：使用统计检验方法（如t检验）来检验参数估计的显著性,
判断自变量与因变量之间是否存在显著的线性关系。 -
模型拟合度评估：使用拟合度指标（如<code>R-squared</code>、调整<code>R-squared</code>等）来评估模型的拟合程度,
判断模型对数据的解释能力。</p>
<p>参数估计决定了模型对数据的拟合程度和预测能力。通过最小二乘法,
可以有效地估计模型的参数, 并通过各种评价方法来验证模型的有效性。然而,
在应用中需要注意参数估计的局限性, 并结合实际情况进行适当的调整和处理,
确保模型的准确性和可靠性。</p>
<p>用于评估模型性能的指标包括: - 均方误差（Mean Squared Error, MSE） -
均方根误差（Root Mean Squared Error, RMSE) - 平均绝对误差 (Mean Absolute
Error, MAE) 等</p>
<p>评估指标是用来衡量模型预测结果与真实观测值之间的差异程度，从而评估模型的性能。
在线性回归中,
常用的评估指标包括均方误差（MSE）、均方根误差（RMSE）和平均绝对误差（MAE）。</p>
<h3 id="均方误差-mse">均方误差 (MSE)</h3>
<p>均方误差是预测值与真实值之间差异的平方的平均值： <span class="math display">\[
M S E=\frac{1}{n} \sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2
\]</span></p>
<p>其中: - <span class="math inline">\(n\)</span> 是样本数量; - <span class="math inline">\(y_i\)</span> 是第 <span class="math inline">\(i\)</span> 个观测值的真实值; - <span class="math inline">\(\hat{y}_i\)</span> 是第 <span class="math inline">\(i\)</span> 个观测值的预测值。</p>
<h3 id="均方根误差rmse">均方根误差（RMSE）</h3>
<p>均方根误差是均方误差的平方根, 用来衡量预测值与真实值之间的平均偏差:
<span class="math display">\[
R M S E=\sqrt{M S E}=\sqrt{\frac{1}{n}
\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2}
\]</span></p>
<h3 id="平均绝对误差-mae">平均绝对误差 (MAE)</h3>
<p>平均绝对误差是预测值与真实值之间差异的绝对值的平均值:</p>
<p><span class="math display">\[
M A E=\frac{1}{n} \sum_{i=1}^n\left|y_i-\hat{y}_i\right|
\]</span></p>
<p>上述举例的方误差（MSE）、均方根误差（RMSE）和平均绝对误差（MAE）计算：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 预测值</span></span><br><span class="line">y_pred = model.predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均方误差（MSE）</span></span><br><span class="line">mse = mean_squared_error(y, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"均方误差 (MSE):"</span>, mse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均方根误差（RMSE）</span></span><br><span class="line">rmse = np.sqrt(mse)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"均方根误差 (RMSE):"</span>, rmse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算平均绝对误差（MAE）</span></span><br><span class="line">mae = mean_absolute_error(y, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"平均绝对误差 (MAE):"</span>, mae)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<pre><code>均方误差 (MSE): 0.36395228249940537
均方根误差 (RMSE): 0.603284578370279
平均绝对误差 (MAE): 0.4390690780172787</code></pre>
<h3 id="推导过程">推导过程</h3>
<p>这些评估指标的推导过程可以通过最小化某些损失函数来实现。以均方误差为例,
可以通过最小化残差平方和来得到参数的估计值。</p>
<p>具体地, 在线性回归中, 的目标是最小化残差平方和: <span class="math display">\[
\text { Residual Sum of Squares (RSS)
}=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2
\]</span></p>
<p>其中 <span class="math inline">\(y_i\)</span> 是真实值, <span class="math inline">\(\hat{y}_i\)</span> 是预测值。通过最小化
RSS，可以得到最优的模型参数估计值。</p>
<h3 id="应用场景">应用场景</h3>
<ul>
<li>均方误差（MSE）：适用于评估模型对异常值的敏感程度。由于MSE计算了预测值与真实值之间的平方差,因此对大的误差给予较大的惩罚,
使得模型更加关注这些大误差, 适用于需要关注所有预测误差的场景。</li>
<li>均方根误差（RMSE）：将均方误差进行平方根处理后得到的指标,
具有与原始数据相同的量纲, 更直观地反映了预测值与真实值的平均偏差,
常用于解释模型的预测误差的平均水平。</li>
<li>平均绝对误差 (MAE) : 对预测值与真实值之间的绝对差值进行求平均,
不考虑差值的正负, 因此更加稳健, 不受异常值的影响,
适用于对异常值敏感的场景。</li>
</ul>
<p>总的来说, 均方误差、均方根误差和平均绝对误差都是常用的评估指标,
各有优缺点, 实际情况中, 要选择合适的指标来评估模型的性能。</p>
<h3 id="r平方r-squared">R平方（R-squared）</h3>
<p><span class="math inline">\(\mathrm{R}\)</span> 平方是一个介于 0 和 1
之间的值,
用来衡量线性回归模型对观测数据的拟合程度。它表示因变量（目标变量）的变异程度中有多少能够被自变量（特征）解释。
<span class="math inline">\(\mathrm{R}\)</span> 平方的计算公式: <span class="math display">\[
R^2=1-\frac{\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2}{\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}
\]</span></p>
<p>其中: - <span class="math inline">\(y_i\)</span> 是第 <span class="math inline">\(i\)</span> 个观测值的真实值（因变量的实际值）。 -
<span class="math inline">\(\hat{y}_i\)</span> 是模型预测的值,
即根据回归模型估计得出的值。 - <span class="math inline">\(\bar{y}\)</span> 是因变量 <span class="math inline">\(y\)</span> 的均值。 <span class="math inline">\(R\)</span> 平方的取值范围为 0 到 1 , 当 <span class="math inline">\(R\)</span> 平方接近 1 时,
表示模型能够很好地解释因变量的变异性, 拟合优度较高。当 <span class="math inline">\(R\)</span> 平方接近 0 时,
表示模型对因变量的解释力较差, 拟合优度较低。</p>
<h3 id="调整后的r平方adjusted-r-squared">调整后的R平方（Adjusted
R-squared）</h3>
<p>调整后的R平方是对 <span class="math inline">\(R\)</span>
平方的一种修正, 考虑了自变量的数量。它用于避免过度拟合问题,
因为增加自变量可能会导致R平方增加, 但也不一定意味着模型更好。</p>
<p>调整后的R平方的计算公式: <span class="math display">\[
\text { Adjusted } R^2=1-\frac{\left(1-R^2\right)(n-1)}{n-p-1}
\]</span></p>
<p>其中: - <span class="math inline">\(R^2\)</span> 是未经调整的 <span class="math inline">\(\mathrm{R}\)</span> 平方。 - <span class="math inline">\(n\)</span> 是样本数量。 - <span class="math inline">\(p\)</span> 是自变量的数量。</p>
<p>调整后的 <span class="math inline">\(R\)</span>
平方通常小于未经调整的 <span class="math inline">\(R\)</span> 平方,
因为它考虑了自变量数量的惩罚。当模型中的自变量不增加解释力时, 调整后的
<span class="math inline">\(R\)</span> 平方值较低。</p>
<p>含义总结： - R平方 表示模型对因变量变异性的解释程度,
数值范围在0到1之间。较高的R平方表示较好的拟合优度,
但高R平方并不一定代表模型的可靠性。 - 调整后的R平方
在R平方的基础上进行修正, 考虑了自变量数量。它通常小于未经调整的R平方,
用于避免过度拟合问题。更高的调整后的R平方表示模型更可靠,
尤其在自变量较多时, 对模型的评估更有帮助。</p>
<p>总而言之, <span class="math inline">\(R\)</span> 平方和调整后的 <span class="math inline">\(R\)</span>
平方是用于评估线性回归模型拟合优度的重要指标,
可以帮助我们了解模型对数据的解释力和可靠性。</p>
<p>上述举例的<code>R-squared</code>、<code>Adjusted R-squared</code>计算代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 R²</span></span><br><span class="line">r2 = r2_score(y, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"决定系数 (R²):"</span>, r2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 Adjusted R²</span></span><br><span class="line">n = <span class="built_in">len</span>(y)  <span class="comment"># 样本数量</span></span><br><span class="line">p = X.shape[<span class="number">1</span>]  <span class="comment"># 自变量数量</span></span><br><span class="line">adj_r2 = <span class="number">1</span> - (<span class="number">1</span> - r2) * (n - <span class="number">1</span>) / (n - p - <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"调整决定系数 (Adjusted R²):"</span>, adj_r2)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>决定系数 (R²): 0.6366981026928232
调整决定系数 (Adjusted R²): 0.6348632446256153</code></pre>
<p>上述举例为简单线性回归模型，现在对多元线性回归模型进行说明。</p>
<h2 id="多元线性回归">4.多元线性回归</h2>
<p>多元线性回归是一种扩展了简单线性回归的模型，在考虑多个自变量的情况下建立与因变量之间的线性关系。</p>
<h3 id="公式推导">公式推导</h3>
<p>多元线性回归模型的数学表达式为: <span class="math display">\[
y=\beta_0+\beta_1 x_1+\beta_2 x_2+\ldots+\beta_r x_r+\epsilon
\]</span></p>
<p>其中: - <span class="math inline">\(y\)</span>
是因变量（要预测的变量）; - <span class="math inline">\(x_1, x_2,
\ldots, x_r\)</span> 是自变量（特征或解释变量）; - <span class="math inline">\(\beta_0\)</span> 是截距（模型的偏置）; - <span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_r\)</span>
是自变量的系数（模型的斜率）; - <span class="math inline">\(\epsilon\)</span> 是误差项,
表示模型无法解释的随机误差。</p>
<h3 id="参数估计过程">参数估计过程</h3>
<p>为了估计多元线性回归模型的参数，可以使用最小二乘法。首先,
将模型的公式写成矩阵形式: <span class="math display">\[
Y=X \beta+\epsilon
\]</span></p>
<p>其中: - <span class="math inline">\(Y\)</span> 是因变量的观测值 <span class="math inline">\((n \times 1\)</span> 的列向量 <span class="math inline">\()\)</span>; - <span class="math inline">\(X\)</span> 是自变量的设计矩阵 <span class="math inline">\((n \times(r+1)\)</span> 的矩阵）,
其中第一列是全1向量, 用来对应截距项; - <span class="math inline">\(\beta\)</span> 是参数向量 <span class="math inline">\(((r+1) \times 1\)</span> 的列向量 <span class="math inline">\()\)</span>; - <span class="math inline">\(\epsilon\)</span> 是误差项 <span class="math inline">\((n \times 1\)</span> 的列向量）。</p>
<p>接下来, 的目标是最小化残差平方和 <span class="math inline">\(\sum_{i=1}^n \epsilon_i^2\)</span>
。将残差向量表示为 <span class="math inline">\(\epsilon=Y-X
\beta\)</span>, 可以得到最小二乘估计为: <span class="math display">\[
\hat{\beta}=\left(X^T X\right)^{-1} X^T Y
\]</span></p>
<p>下面进行代码举例：
使用<code>Iris</code>数据集的前两个特征进行回归，并将第三个特征作为因变量。
- 蓝色点表示实际数据。 - 红色点表示模型预测的数据。 -
黄色平面表示回归模型拟合的平面。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, r2_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载Iris数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data[:, :<span class="number">2</span>]  <span class="comment"># 使用前2个特征</span></span><br><span class="line">y = iris.data[:, <span class="number">2</span>].reshape(-<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 使用第3个特征作为因变量</span></span><br><span class="line"></span><br><span class="line">X = X[:<span class="number">150</span>]</span><br><span class="line">y = y[:<span class="number">150</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X, y)</span><br><span class="line">y_pred = model.predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均方误差</span></span><br><span class="line">mse = mean_squared_error(y, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"均方误差 (MSE):"</span>, mse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均方根误差（RMSE）</span></span><br><span class="line">rmse = np.sqrt(mse)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"均方根误差 (RMSE):"</span>, rmse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算平均绝对误差（MAE）</span></span><br><span class="line">mae = mean_absolute_error(y, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"平均绝对误差 (MAE):"</span>, mae)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 R²</span></span><br><span class="line">r2 = r2_score(y, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"决定系数 (R²):"</span>, r2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算调整后的 R²</span></span><br><span class="line">n = y.shape[<span class="number">0</span>]  <span class="comment"># 样本数量</span></span><br><span class="line">p = X.shape[<span class="number">1</span>]  <span class="comment"># 自变量数量</span></span><br><span class="line">adj_r2 = <span class="number">1</span> - (<span class="number">1</span> - r2) * (n - <span class="number">1</span>) / (n - p - <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"调整决定系数 (Adjusted R²):"</span>, adj_r2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">15</span>, <span class="number">15</span>))</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</span><br><span class="line">ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], y, color=<span class="string">'b'</span>, label=<span class="string">'实际值'</span>)</span><br><span class="line">ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], y_pred, color=<span class="string">'r'</span>, label=<span class="string">'预测值'</span>)</span><br><span class="line"></span><br><span class="line">xx1, xx2 = np.meshgrid(np.linspace(X[:, <span class="number">0</span>].<span class="built_in">min</span>(), X[:, <span class="number">0</span>].<span class="built_in">max</span>(), <span class="number">100</span>), np.linspace(X[:, <span class="number">1</span>].<span class="built_in">min</span>(), X[:, <span class="number">1</span>].<span class="built_in">max</span>(), <span class="number">100</span>))</span><br><span class="line">yy = model.intercept_ + model.coef_[<span class="number">0</span>][<span class="number">0</span>]*xx1 + model.coef_[<span class="number">0</span>][<span class="number">1</span>]*xx2</span><br><span class="line">ax.plot_surface(xx1, xx2, yy, alpha=<span class="number">0.5</span>, color=<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'花萼长度 (cm)'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'花萼宽度 (cm)'</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">'花瓣长度 (cm)'</span>)</span><br><span class="line">ax.set_title(<span class="string">'多元线性回归的3D图'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<pre><code>均方误差 (MSE): 0.40957831218050467
均方根误差 (RMSE): 0.6399830561667275
平均绝对误差 (MAE): 0.5162246726456129
决定系数 (R²): 0.8676860089345194
调整决定系数 (Adjusted R²): 0.865885818579887</code></pre>
<p><img src="../../../../images/机器学习概论/output_13_1.png"></p>
<h2 id="特征选择">5.特征选择</h2>
<p>特征选择可以帮助从⼤量的⾃变量中挑选出最具影响⼒的变量，以提⾼模型的预测性能、减少过拟合的⻛险并简
化模型的解释。 ### 多重共线性（Multicollinearity）
多重共线性是指⾃变量之间存在⾼度相关性或线性依赖关系的情况。</p>
<p><strong>多重共线性可能会导致以下问题</strong> 1.
不稳定的估计：多重共线性会导致回归系数估计变得不稳定。这意味着⼩的数据变动或微⼩的变量选择变化都可能导致回归系数的⼤幅度变化，使得参数估计不可靠。
2.
难以解释效果：多重共线性使得很难分离各⾃⾃变量对因变量的独⽴效应，因为它们之间的效应不再明确。这会降低模型的解释能⼒。
3.
统计检验不准确：多重共线性会导致回归模型的统计检验不准确，如t检验和F检验。这可能会导致错误的结论，例如错误地认为某些⾃变量对因变量没有显著影响。
4.
过度拟合：多重共线性可以导致过度拟合，因为模型可能会在⾃变量之间寻找微⼩的变化，从⽽试图解释由于共线性引起的噪声。</p>
<p><strong>检测多重共线性</strong> 1.
相关系数分析：通过计算⾃变量之间的相关系数矩阵，可以初步了解⾃变量之间是否存在⾼度相关性。相关系数接近于1表示⾼度相关。
2.
⽅差膨胀因⼦（VIF）：VIF⽤于衡量每个⾃变量与其他⾃变量的相关性程度。VIF越⼤，表示共线性越严重。通常，VIF⼤于10或更⾼的⾃变量可能需要考虑去除或合并。
3.
主成分分析（PCA）：PCA可以将相关的⾃变量合并成新的⽆关⾃变量，从⽽减少共线性的影响。但这会导致模型的解释变得更加复杂。
4.
逐步回归：逐步回归⽅法允许逐渐添加或删除⾃变量，以找到最佳模型。在逐步回归中，会考虑每个⾃变量的贡献，从⽽减少共线性引起的问题。
5.
合并⾃变量：如果多个⾃变量之间⾼度相关，可以考虑将它们合并成⼀个新的⾃变量或使⽤其平均值来代替。这样可以减少模型中的共线性。</p>
<p>下面我们使用<code>Iris</code>的4个特征作为自变量、种类作为因变量进行多重共线性分析。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载Iris数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data[:<span class="number">150</span>, :<span class="number">4</span>]  </span><br><span class="line">y = iris.target[:<span class="number">150</span>]  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建DataFrame</span></span><br><span class="line">df = pd.DataFrame(X, columns=iris.feature_names)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相关系数矩阵</span></span><br><span class="line">correlation_matrix = df.corr()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"相关系数矩阵："</span>)</span><br><span class="line"><span class="built_in">print</span>(correlation_matrix)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>相关系数矩阵：
                   sepal length (cm)  sepal width (cm)  petal length (cm)  \
sepal length (cm)           1.000000         -0.117570           0.871754   
sepal width (cm)           -0.117570          1.000000          -0.428440   
petal length (cm)           0.871754         -0.428440           1.000000   
petal width (cm)            0.817941         -0.366126           0.962865   

                   petal width (cm)  
sepal length (cm)          0.817941  
sepal width (cm)          -0.366126  
petal length (cm)          0.962865  
petal width (cm)           1.000000  </code></pre>
<p><strong>⽅差膨胀因⼦（VIF）</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> statsmodels.stats.outliers_influence <span class="keyword">import</span> variance_inflation_factor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算VIF</span></span><br><span class="line">vif_data = pd.DataFrame()</span><br><span class="line">vif_data[<span class="string">'特征'</span>] = df.columns</span><br><span class="line">vif_data[<span class="string">'VIF'</span>] = [variance_inflation_factor(df.values, i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(df.shape[<span class="number">1</span>])]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n方差膨胀因子（VIF）："</span>)</span><br><span class="line"><span class="built_in">print</span>(vif_data)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>方差膨胀因子（VIF）：
                  特征         VIF
0  sepal length (cm)  262.969348
1   sepal width (cm)   96.353292
2  petal length (cm)  172.960962
3   petal width (cm)   55.502060</code></pre>
<p><strong>主成分分析（PCA）</strong></p>
<ol type="1">
<li>数据标准化和中心化</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 对每个特征求平均值</span></span><br><span class="line">mean_vec = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"特征均值："</span>, mean_vec)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 原来的数据减去平均值得到新的中心化之后的数据</span></span><br><span class="line">centered_X = X - mean_vec</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n中心化后的数据：\n"</span>, centered_X[:<span class="number">5</span>])  <span class="comment"># 只显示前5行</span></span><br></pre></td></tr></tbody></table></figure>
<pre><code>特征均值： [5.84333333 3.05733333 3.758      1.19933333]

中心化后的数据：
 [[-0.74333333  0.44266667 -2.358      -0.99933333]
 [-0.94333333 -0.05733333 -2.358      -0.99933333]
 [-1.14333333  0.14266667 -2.458      -0.99933333]
 [-1.24333333  0.04266667 -2.258      -0.99933333]
 [-0.84333333  0.54266667 -2.358      -0.99933333]]</code></pre>
<ol start="2" type="1">
<li>计算协方差矩阵</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 计算协方差矩阵</span></span><br><span class="line">cov_matrix = np.cov(centered_X.T)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n协方差矩阵：\n"</span>, cov_matrix)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>协方差矩阵：
 [[ 0.68569351 -0.042434    1.27431544  0.51627069]
 [-0.042434    0.18997942 -0.32965638 -0.12163937]
 [ 1.27431544 -0.32965638  3.11627785  1.2956094 ]
 [ 0.51627069 -0.12163937  1.2956094   0.58100626]]</code></pre>
<ol start="3" type="1">
<li>计算特征值和特征向量</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 计算协方差矩阵的特征值和特征向量</span></span><br><span class="line">eig_vals, eig_vecs = np.linalg.eig(cov_matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n特征值：\n"</span>, eig_vals)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n特征向量：\n"</span>, eig_vecs)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>特征值：
 [4.22824171 0.24267075 0.0782095  0.02383509]

特征向量：
 [[ 0.36138659 -0.65658877 -0.58202985  0.31548719]
 [-0.08452251 -0.73016143  0.59791083 -0.3197231 ]
 [ 0.85667061  0.17337266  0.07623608 -0.47983899]
 [ 0.3582892   0.07548102  0.54583143  0.75365743]]</code></pre>
<ol start="4" type="1">
<li>选择主成分和投影矩阵</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 对特征值按照降序排列，并选择前两个主成分</span></span><br><span class="line">sorted_indices = np.argsort(eig_vals)[::-<span class="number">1</span>]</span><br><span class="line">sorted_eig_vals = eig_vals[sorted_indices]</span><br><span class="line">sorted_eig_vecs = eig_vecs[:, sorted_indices]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择前两个主成分</span></span><br><span class="line">n_components = <span class="number">2</span></span><br><span class="line">selected_eig_vecs = sorted_eig_vecs[:, :n_components]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n选择的特征向量（投影矩阵）：\n"</span>, selected_eig_vecs)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>选择的特征向量（投影矩阵）：
 [[ 0.36138659 -0.65658877]
 [-0.08452251 -0.73016143]
 [ 0.85667061  0.17337266]
 [ 0.3582892   0.07548102]]</code></pre>
<ol start="5" type="1">
<li>求出降维后的数据</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 根据投影矩阵求出降维后的数据</span></span><br><span class="line">X_pca = centered_X.dot(selected_eig_vecs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n降维后的数据：\n"</span>, X_pca[:<span class="number">5</span>])  <span class="comment"># 只显示前5行</span></span><br></pre></td></tr></tbody></table></figure>
<pre><code>降维后的数据：
 [[-2.68412563 -0.31939725]
 [-2.71414169  0.17700123]
 [-2.88899057  0.14494943]
 [-2.74534286  0.31829898]
 [-2.72871654 -0.32675451]]</code></pre>
<p><strong>也可直接使用<code>Scikit-Learn</code></strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行PCA</span></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)  <span class="comment">#降到2维</span></span><br><span class="line">X_pca = pca.fit_transform(X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n主成分分析结果："</span>)</span><br><span class="line"><span class="built_in">print</span>(X_pca[:<span class="number">5</span>])  <span class="comment"># 只显示前5行</span></span><br></pre></td></tr></tbody></table></figure>
<pre><code>主成分分析结果：
[[-2.68412563  0.31939725]
 [-2.71414169 -0.17700123]
 [-2.88899057 -0.14494943]
 [-2.74534286 -0.31829898]
 [-2.72871654  0.32675451]]</code></pre>
<p>数据本身是4维的，降维后为2维</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sklearn.decomposition <span class="keyword">as</span> dp</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载Iris数据集，x表示数据集中的属性数据，y表示数据标签</span></span><br><span class="line">x, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载PCA算法，设置降维后主成分数目为2</span></span><br><span class="line">pca = dp.PCA(n_components=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对原始数据进行降维，保存在reduced_x中</span></span><br><span class="line">reduced_x = pca.fit_transform(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建不同类别的列表来保存降维后的数据点</span></span><br><span class="line">red_x, red_y = [], []</span><br><span class="line">blue_x, blue_y = [], []</span><br><span class="line">green_x, green_y = [], []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按鸢尾花的类别将降维后的数据点保存在不同的列表中</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(reduced_x)):</span><br><span class="line">    <span class="keyword">if</span> y[i] == <span class="number">0</span>:</span><br><span class="line">        red_x.append(reduced_x[i][<span class="number">0</span>])</span><br><span class="line">        red_y.append(reduced_x[i][<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">elif</span> y[i] == <span class="number">1</span>:</span><br><span class="line">        blue_x.append(reduced_x[i][<span class="number">0</span>])</span><br><span class="line">        blue_y.append(reduced_x[i][<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        green_x.append(reduced_x[i][<span class="number">0</span>])</span><br><span class="line">        green_y.append(reduced_x[i][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.scatter(red_x, red_y, c=<span class="string">'r'</span>, marker=<span class="string">'x'</span>, label=<span class="string">'类别 0'</span>)</span><br><span class="line">plt.scatter(blue_x, blue_y, c=<span class="string">'b'</span>, marker=<span class="string">'D'</span>, label=<span class="string">'类别 1'</span>)</span><br><span class="line">plt.scatter(green_x, green_y, c=<span class="string">'g'</span>, marker=<span class="string">'.'</span>, label=<span class="string">'类别 2'</span>)</span><br><span class="line">plt.title(<span class="string">'Iris数据集PCA降维'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'主成分 1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'主成分 2'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p>​<br>
<img src="../../../../images/机器学习概论/output_32_0.png"> ​</p>
<p><strong>通过观察图中不同颜色点的分布，可以发现</strong>： -
Iris-setosa (类别 0 )
与其他两类在降维后的空间中分布较为独立，显示出明显的分离。 -
Iris-versicolor (类别1) 和Iris-virginica (类别2)
有部分重叠，表明这两类在原始特征空间中的某些属性比较接近。</p>
<p>下面使用加州房价数据集进行再一次尝试。
使用不同的特征组合进行比较。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing</span><br><span class="line"></span><br><span class="line">housing = fetch_california_housing()</span><br><span class="line">X = housing.data</span><br><span class="line">Y = housing.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割训练集和测试集</span></span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数，用于评估模型的性能</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_model</span>(<span class="params">X_train, X_test, Y_train, Y_test</span>):</span><br><span class="line">    model = LinearRegression()</span><br><span class="line">    model.fit(X_train, Y_train)</span><br><span class="line">    Y_pred = model.predict(X_test)</span><br><span class="line">    mse = mean_squared_error(Y_test, Y_pred)</span><br><span class="line">    <span class="keyword">return</span> mse, Y_pred</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型评估</span></span><br><span class="line">all_features_mse, _ = evaluate_model(X_train, X_test, Y_train, Y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"使用所有特征的MSE:"</span>, all_features_mse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择不同的自变量组合进行建模</span></span><br><span class="line">selected_features = [[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">0</span>, <span class="number">4</span>, <span class="number">5</span>]]  <span class="comment"># 不同的自变量组合</span></span><br><span class="line">mse_values = []</span><br><span class="line"><span class="keyword">for</span> i, features <span class="keyword">in</span> <span class="built_in">enumerate</span>(selected_features):</span><br><span class="line">    X_train_selected = X_train[:, features]</span><br><span class="line">    X_test_selected = X_test[:, features]</span><br><span class="line">    mse, Y_pred = evaluate_model(X_train_selected, X_test_selected, Y_train, Y_test)</span><br><span class="line">    mse_values.append(mse)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 可视化预测结果</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">    plt.scatter(Y_test, Y_pred, label=<span class="string">f'特征组合: <span class="subst">{features}</span>'</span>, s=<span class="number">1</span>)</span><br><span class="line">    plt.plot([Y_test.<span class="built_in">min</span>(), Y_test.<span class="built_in">max</span>()], [Y_test.<span class="built_in">min</span>(), Y_test.<span class="built_in">max</span>()], <span class="string">'k--'</span>, lw=<span class="number">2</span>)  <span class="comment"># 对角线</span></span><br><span class="line">    plt.xlabel(<span class="string">'真实值'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'预测值'</span>)</span><br><span class="line">    plt.title(<span class="string">f'真实值 vs 预测值 使用特征组合: <span class="subst">{features}</span>'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印不同自变量组合的 MSE 值</span></span><br><span class="line"><span class="keyword">for</span> i, features <span class="keyword">in</span> <span class="built_in">enumerate</span>(selected_features):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"使用特征组合 <span class="subst">{features}</span> 的MSE: <span class="subst">{mse_values[i]}</span>"</span>)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<pre><code>使用所有特征的MSE: 0.5558915986952422</code></pre>
<p><img src="../../../../images/机器学习概论/output_35_1.png"></p>
<p><img src="../../../../images/机器学习概论/output_35_2.png"></p>
<p><img src="../../../../images/机器学习概论/output_35_3.png"></p>
<p><img src="../../../../images/机器学习概论/output_35_4.png"></p>
<pre><code>使用特征组合 [0, 1] 的MSE: 0.6629874283048177
使用特征组合 [1, 2, 3] 的MSE: 1.1293479456543194
使用特征组合 [2, 3, 4] 的MSE: 1.1724183557562893
使用特征组合 [0, 4, 5] 的MSE: 0.7070316559271143</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行PCA</span></span><br><span class="line">pca = PCA(n_components=<span class="number">3</span>)</span><br><span class="line">X_pca = pca.fit_transform(X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"\n主成分分析结果："</span>)</span><br><span class="line"><span class="built_in">print</span>(X_pca[:<span class="number">5</span>])  <span class="comment"># 只显示前5行</span></span><br></pre></td></tr></tbody></table></figure>
<pre><code>主成分分析结果：
[[-1.10351265e+03  8.56663624e+00 -7.74104267e-01]
 [ 9.75541244e+02 -4.67041774e+00 -1.02568060e+00]
 [-9.29549908e+02  2.00346509e+01 -1.97273701e+00]
 [-8.67550048e+02  2.03314414e+01 -2.31492872e+00]
 [-8.60550411e+02  2.03288266e+01 -2.69515779e+00]]</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 计算VIF</span></span><br><span class="line">X_df = pd.DataFrame(X, columns=housing.feature_names)</span><br><span class="line">vif_data = pd.DataFrame()</span><br><span class="line">vif_data[<span class="string">'特征'</span>] = X_df.columns</span><br><span class="line">vif_data[<span class="string">'VIF'</span>] = [variance_inflation_factor(X_df.values, i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X_df.shape[<span class="number">1</span>])]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"方差膨胀因子（VIF）："</span>)</span><br><span class="line"><span class="built_in">print</span>(vif_data)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>方差膨胀因子（VIF）：
           特征         VIF
0      MedInc   11.511140
1    HouseAge    7.195917
2    AveRooms   45.993601
3   AveBedrms   43.590314
4  Population    2.935745
5    AveOccup    1.095243
6    Latitude  559.874071
7   Longitude  633.711654</code></pre>
<h2 id="正则化技术">6.正则化技术</h2>
<p>正则化技术在机器学习中被广泛应用于控制模型的复杂度,
避免过拟合问题。L1正则化（也称为Lasso回归）和L2
正则化（也称为岭回归）是常用的正则化方法,
它们分别通过添加L1范数和L2范数的惩罚项来限制模型参数的大小。</p>
<h3 id="l1正则化lasso回归">L1正则化（Lasso回归）</h3>
<p>L1正则化通过添加参数向量的L1范数作为惩罚项, 其优化目标可以表示为:
<span class="math display">\[
\operatorname{minimize} \quad \mathrm{MSE}+\lambda
\sum_{j=1}^p\left|\beta_j\right|
\]</span></p>
<p>其中, MSE 是均方误差（Mean Squared Error）， <span class="math inline">\(\lambda\)</span> 是正则化参数,
控制正则化项的影响力, 较⼤的 <span class="math inline">\(\lambda\)</span> 值会导致更多的系数变为零，<span class="math inline">\(\beta_j\)</span> 是模型的第 <span class="math inline">\(j\)</span>个参数。</p>
<h3 id="l2正则化岭回归">L2正则化（岭回归）</h3>
<p>L2正则化通过添加参数向量的L2范数的平方作为惩罚项,
其优化目标可以表示为: <span class="math display">\[
\operatorname{minimize} \quad \mathrm{MSE}+\lambda \sum_{j=1}^p
\beta_j^2
\]</span></p>
<p>其中, MSE 是均方误差（Mean Squared Error）， <span class="math inline">\(\lambda\)</span> 是正则化参数,
控制正则化项的影响力, 较⼤的 <span class="math inline">\(\lambda\)</span> 值会导致系数趋向于零，<span class="math inline">\(\beta_j\)</span> 是模型的第 <span class="math inline">\(j\)</span>个参数。</p>
<p>下面举例使用一个多项式函数作为目标函数,
然后分别演示不使用正则化、使用L1正则化
（Lasso回归）和使用L2正则化（Ridge回归）的情况。</p>
<p>代码中, 生成一个具有噪声的多项式数据集,
并拟合三种不同类型的模型。</p>
<p>最后，绘制一个拟合曲线以及观察它们之间的差异。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures, StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, Lasso, Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成多项式数据集</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = np.sort(<span class="number">5</span> * np.random.rand(<span class="number">70</span>, <span class="number">1</span>), axis=<span class="number">0</span>)</span><br><span class="line">y = np.sin(X).ravel() + np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, X.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义不同的多项式阶数</span></span><br><span class="line">degrees = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">15</span>]  </span><br><span class="line">models = [(<span class="string">'无正则化'</span>, <span class="string">'r'</span>, <span class="string">'-'</span>), </span><br><span class="line">          (<span class="string">'L1 正则化 (Lasso)'</span>, <span class="string">'g'</span>, <span class="string">'--'</span>), </span><br><span class="line">          (<span class="string">'L2 正则化 (Ridge)'</span>, <span class="string">'b'</span>, <span class="string">'-.'</span>)]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">14</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, (name, color, linestyle) <span class="keyword">in</span> <span class="built_in">enumerate</span>(models):</span><br><span class="line">    ax = plt.subplot(<span class="number">1</span>, <span class="built_in">len</span>(models), i + <span class="number">1</span>)</span><br><span class="line">    plt.setp(ax, xticks=(), yticks=())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将特征进行多项式转换并标准化</span></span><br><span class="line">    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=<span class="literal">False</span>)</span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> name == <span class="string">'L1 正则化 (Lasso)'</span>:</span><br><span class="line">        model = make_pipeline(polynomial_features, scaler, Lasso(alpha=<span class="number">0.001</span>, max_iter=<span class="number">10000</span>))</span><br><span class="line">    <span class="keyword">elif</span> name == <span class="string">'L2 正则化 (Ridge)'</span>:</span><br><span class="line">        model = make_pipeline(polynomial_features, scaler, Ridge(alpha=<span class="number">1.0</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model = make_pipeline(polynomial_features, scaler, LinearRegression())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 拟合模型</span></span><br><span class="line">    model.fit(X, y)</span><br><span class="line">    y_pred = model.predict(X)</span><br><span class="line">    mse = mean_squared_error(y, y_pred)</span><br><span class="line">    </span><br><span class="line">    plt.plot(X, y, <span class="string">'o'</span>, color=<span class="string">'cornflowerblue'</span>, label=<span class="string">"实际值"</span>)</span><br><span class="line">    plt.plot(X, y_pred, color=color, linestyle=linestyle, linewidth=<span class="number">2</span>,</span><br><span class="line">             label=<span class="string">"{} (均方误差: {:.2f})"</span>.<span class="built_in">format</span>(name, mse))</span><br><span class="line">    </span><br><span class="line">    plt.title(<span class="string">"多项式阶数 {}\n均方误差 = {:.2e}"</span>.<span class="built_in">format</span>(degrees[i], mse), fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"x"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"y"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">"best"</span>, fontsize=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p>​<br>
<img src="../../../../images/机器学习概论/output_39_0.png"> ​</p>
<h3 id="lasso回归岭回归与普通线性回归之间的区别">Lasso回归、岭回归与普通线性回归之间的区别</h3>
<ol type="1">
<li>正则化类型:
<ul>
<li>普通线性回归没有正则化项，其目标是最小化残差平方和，不对回归系数进行任何约束。</li>
<li>岭回归引入了 <span class="math inline">\(L 2\)</span> 正则化项,
其目标是最小化残差平方和与回归系数的平方和之和,
以限制回归系数的大小。</li>
<li>Lasso回归引入了 L1 正则化项,
其目标是最小化残差平方和与回归系数的绝对值之和,
以促使一些回归系数变为零, 实现特征选择。</li>
</ul></li>
<li>系数的性质：
<ul>
<li>普通线性回归的系数可以是任意实数, 没有限制。</li>
<li>岭回归的系数趋向于缩小但不会变为零, 因此不会进行特征选择。</li>
<li>Lasso回归的系数可以变为零, 从而实现了自动特征选择,
使模型更稀疏。</li>
</ul></li>
<li>解决的问题：
<ul>
<li>普通线性回归通常用于建模和预测,
但在存在多重共线性时容易过拟合。</li>
<li>岭回归主要用于解决多重共线性问题, 可以稳定估计,
但不进行特征选择。</li>
<li>Lasso 回归用于解决多重共线性问题, 并且可以进行特征选择,
有助于提取最重要的特征。</li>
</ul></li>
<li>正则化参数:
<ul>
<li>在岭回归和Lasso回归中, 正则化参数 (a) 用于控制正则化的强度。较大的
a值会导致更强的正则化, 推动回归系数向零缩小。</li>
<li>普通线性回归没有正则化参数。</li>
</ul></li>
<li>特征选择:
<ul>
<li>普通线性回归不进行特征选择，使用所有特征。</li>
<li>岭回归倾向于减小回归系数但不会消除它们, 不进行特征选择。</li>
<li>Lasso 回归可以将某些系数压缩为零, 实现特征选择, 使模型更稀疏。</li>
</ul></li>
</ol>
<h1 id="二梯度下降法">二、梯度下降法</h1>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义目标函数及其梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span>*x*y+np.sin(x)+np.sin(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grad_f</span>(<span class="params">x, y</span>):</span><br><span class="line">    grad_x = <span class="number">2</span> * x + <span class="number">50</span> * np.sin(x) * np.cos(x)</span><br><span class="line">    grad_y = <span class="number">2</span> * y + <span class="number">50</span> * np.sin(y) * np.cos(y)</span><br><span class="line">    <span class="keyword">return</span> np.array([grad_x, grad_y])</span><br><span class="line">alpha = <span class="number">0.1</span>  </span><br><span class="line">start_point = np.array([<span class="number">5.0</span>, <span class="number">5.0</span>])  </span><br><span class="line">epochs = <span class="number">20</span>  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降过程</span></span><br><span class="line">point = start_point</span><br><span class="line">points = [point]</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    point = point - alpha * grad_f(point[<span class="number">0</span>], point[<span class="number">1</span>])</span><br><span class="line">    points.append(point)</span><br><span class="line">x_range = np.linspace(-<span class="number">7</span>, <span class="number">7</span>, <span class="number">400</span>)</span><br><span class="line">y_range = np.linspace(-<span class="number">7</span>, <span class="number">7</span>, <span class="number">400</span>)</span><br><span class="line">X, Y = np.meshgrid(x_range, y_range)</span><br><span class="line">Z = f(X, Y)</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</span><br><span class="line">ax.plot_surface(X, Y, Z, cmap=<span class="string">'viridis'</span>, alpha=<span class="number">0.6</span>)</span><br><span class="line">points = np.array(points)</span><br><span class="line">Z_points = f(points[:, <span class="number">0</span>], points[:, <span class="number">1</span>])</span><br><span class="line">ax.plot(points[:, <span class="number">0</span>], points[:, <span class="number">1</span>], Z_points, color=<span class="string">'red'</span>, marker=<span class="string">'o'</span>, linestyle=<span class="string">'-'</span>, label=<span class="string">'梯度下降过程'</span>)</span><br><span class="line"><span class="keyword">for</span> i, (x, y, z) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(points[:, <span class="number">0</span>], points[:, <span class="number">1</span>], Z_points)):</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span> <span class="keyword">or</span> i == epochs:</span><br><span class="line">        ax.text(x, y, z, <span class="string">f'<span class="subst">{i}</span>'</span>, fontsize=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">'$x$'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'$y$'</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">'$f(x, y)$'</span>)</span><br><span class="line">ax.set_title(<span class="string">'梯度下降法示意图'</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">ax.view_init(elev=<span class="number">50</span>, azim=-<span class="number">60</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p>​<br>
<img src="../../../../images/机器学习概论/output_42_0.png"> ​</p>
<p>梯度下降法(Gradient
Descent)是一种算法，但不是像多元线性回归那样是一个具体做回归任务的算法，而是一个非常通用的优化算法来帮助一些机器学习算法求解出最优解的，所谓的通用就是很多机器学习算法都是用它，甚至深度学习也是用它来求解最优解。</p>
<h2 id="梯度下降法的思想">1.梯度下降法的思想</h2>
<ol type="1">
<li><p>初始随机猜测
在梯度下降法中，首先需要对模型参数进行初始化。通常，参数向量 <span class="math inline">\(\theta=\left[W_1, W_2, \ldots, W_n\right]\)</span>
是通过随机选择的初始值。这个过程类似于在一个高维空间中随机选择一个起点。</p></li>
<li><p>计算预测值与损失 通过当前的参数向量 <span class="math inline">\(\theta\)</span> ，可以计算模型的预测值 <span class="math inline">\(\hat{y}\)</span>
。例如，对于线性回归模型，可以通过公式 <span class="math inline">\(\hat{y}=\)</span> <span class="math inline">\(X
\theta\)</span> 计算预测值。接着，将预测值 <span class="math inline">\(\hat{y}\)</span> 与真实值 <span class="math inline">\(y\)</span> 进行比较，计算损失函数（如均方误差
MSE），以衡量模型的预测性能。</p></li>
<li><p>计算梯度 梯度是损失函数相对于参数向量 <span class="math inline">\(\theta\)</span>
的一阶导数，表示在当前点上损失函数的最陡上升方向。通过计算梯度，可以确定如何调整参数以使损失函数减小。对于均方误差损失函数，其梯度可以表示为:
<span class="math display">\[
\nabla_\theta L(\theta)=\frac{\partial L(\theta)}{\partial \theta}
\]</span></p></li>
<li><p>参数更新 根据计算得到的梯度，更新参数向量 <span class="math inline">\(\theta\)</span> 。更新公式为: <span class="math display">\[
  \theta_{t+1}=\theta_t-\alpha \nabla_\theta L\left(\theta_t\right)
  \]</span> 其中， <span class="math inline">\(\alpha\)</span>
是学习率，控制每次参数更新的步长。</p></li>
<li><p>迭代
重复上述步骤，逐步调整参数，使得损失函数值不断减小。这个过程持续进行，直到损失函数收玫到一个极小值（或达到预定的迭代次数）。</p></li>
</ol>
<p>梯度下降法的直观类比
可以将梯度下降法类比为一个人在山谷中行走，试图找到最低点。在每一步中，这个人会评估当前所在位置的坡度，并朝着坡度最陡的下坡方向前进。通过不断地调整方向和步伐，这个人最终会到达谷底，即找到最小损失的参数组合。</p>
<h1 id="未完待续...">未完待续...</h1>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://studyincau.github.io">小楼一夜听春雨 &amp; Rico</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://studyincau.github.io/2024/05/28/ji-qi-xue-xi-gai-lun/">https://studyincau.github.io/2024/05/28/ji-qi-xue-xi-gai-lun/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://studyincau.github.io" target="_blank">StudyinCAU</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/../images/ML.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/01/hua-tu/" title="Python可视化"><img class="cover" src="/../images/seaborn.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Python可视化</div></div></a></div><div class="next-post pull-right"><a href="/2024/05/18/shu-li-tong-ji-6-4-bei-xie-si-gu-ji/" title="第四节 贝叶斯估计"><img class="cover" src="/../images/background46.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">第四节 贝叶斯估计</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/04/28/di-01-zhang-tong-ji-xue-xi-fang-fa-gai-lun/" title="第1章 统计学习方法概论"><img class="cover" src="/../images/ML.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-28</div><div class="title">第1章 统计学习方法概论</div></div></a></div><div><a href="/2024/04/28/di-02-zhang-gan-zhi-ji/" title="第2章 感知机"><img class="cover" src="/../images/ML.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-28</div><div class="title">第2章 感知机</div></div></a></div><div><a href="/2024/04/29/di-03-zhang-k-jin-lin-fa/" title="第3章 k近邻法"><img class="cover" src="/../images/ML.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-29</div><div class="title">第3章 k近邻法</div></div></a></div><div><a href="/2024/04/29/di-4-zhang-po-su-bei-xie-si/" title="第4章 朴素贝叶斯"><img class="cover" src="/../images/ML.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-29</div><div class="title">第4章 朴素贝叶斯</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">小楼一夜听春雨 &amp; Rico</div><div class="author-info__description">Study</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">124</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">207</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">53</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/StudyinCAU"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/StudyinCAU" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:3187248635@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这是一个分享学习笔记(资料)的网站，欢迎一起学习、交流。如果你有好的文章也想分享，可以发邮箱(邮箱在公告上方~)[若出现渲染问题或加载过慢，推荐用谷歌或Edge浏览器打开]</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA"><span class="toc-text">机器学习概论</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">一、线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E5%92%8C%E5%81%87%E8%AE%BE"><span class="toc-text">1.线性回归模型的基本原理和假设</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="toc-text">2.参数估计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-text">最小二乘法的原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-text">3.评估指标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE-mse"><span class="toc-text">均方误差 (MSE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E6%A0%B9%E8%AF%AF%E5%B7%AErmse"><span class="toc-text">均方根误差（RMSE）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AE-mae"><span class="toc-text">平均绝对误差 (MAE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B"><span class="toc-text">推导过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#r%E5%B9%B3%E6%96%B9r-squared"><span class="toc-text">R平方（R-squared）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E6%95%B4%E5%90%8E%E7%9A%84r%E5%B9%B3%E6%96%B9adjusted-r-squared"><span class="toc-text">调整后的R平方（Adjusted
R-squared）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">4.多元线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="toc-text">公式推导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E8%BF%87%E7%A8%8B"><span class="toc-text">参数估计过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-text">5.特征选择</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-text">6.正则化技术</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#l1%E6%AD%A3%E5%88%99%E5%8C%96lasso%E5%9B%9E%E5%BD%92"><span class="toc-text">L1正则化（Lasso回归）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#l2%E6%AD%A3%E5%88%99%E5%8C%96%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="toc-text">L2正则化（岭回归）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lasso%E5%9B%9E%E5%BD%92%E5%B2%AD%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%99%AE%E9%80%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">Lasso回归、岭回归与普通线性回归之间的区别</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-text">二、梯度下降法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E6%80%9D%E6%83%B3"><span class="toc-text">1.梯度下降法的思想</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%AA%E5%AE%8C%E5%BE%85%E7%BB%AD..."><span class="toc-text">未完待续...</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/06/25/di-09-zhang-dba-ming-ling/" title="第9章 DBA命令"><img src="/../images/background5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第9章 DBA命令"/></a><div class="content"><a class="title" href="/2024/06/25/di-09-zhang-dba-ming-ling/" title="第9章 DBA命令">第9章 DBA命令</a><time datetime="2024-06-25T05:21:18.000Z" title="发表于 2024-06-25 13:21:18">2024-06-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/25/di-08-zhang-shi-wu/" title="第8章 事务"><img src="/../images/background5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第8章 事务"/></a><div class="content"><a class="title" href="/2024/06/25/di-08-zhang-shi-wu/" title="第8章 事务">第8章 事务</a><time datetime="2024-06-25T05:21:12.000Z" title="发表于 2024-06-25 13:21:12">2024-06-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/25/di-07-zhang-shi-tu/" title="第7章 视图"><img src="/../images/background5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第7章 视图"/></a><div class="content"><a class="title" href="/2024/06/25/di-07-zhang-shi-tu/" title="第7章 视图">第7章 视图</a><time datetime="2024-06-25T05:20:26.000Z" title="发表于 2024-06-25 13:20:26">2024-06-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/25/di-06-zhang-san-fan-shi/" title="第6章 三范式"><img src="/../images/background5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第6章 三范式"/></a><div class="content"><a class="title" href="/2024/06/25/di-06-zhang-san-fan-shi/" title="第6章 三范式">第6章 三范式</a><time datetime="2024-06-25T05:19:58.000Z" title="发表于 2024-06-25 13:19:58">2024-06-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/25/di-05-zhang-biao-xiang-guan/" title="第5章 表相关"><img src="/../images/background5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第5章 表相关"/></a><div class="content"><a class="title" href="/2024/06/25/di-05-zhang-biao-xiang-guan/" title="第5章 表相关">第5章 表相关</a><time datetime="2024-06-25T05:19:30.000Z" title="发表于 2024-06-25 13:19:30">2024-06-25</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By 小楼一夜听春雨 & Rico</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'xFi1vMHzzqw1qFKQbyK4WtAo-gzGzoHsz',
      appKey: 'hGyKCk4AFAr9aPSCHioyb0hH',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !true) {
    if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><link rel="stylesheet" href="/css/title.css"><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>